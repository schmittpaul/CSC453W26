[
  {
    "objectID": "slosh.html#slosh",
    "href": "slosh.html#slosh",
    "title": "CSC 453",
    "section": "SLOsh",
    "text": "SLOsh\n\nThis assignment should be a refresher of 357 topics\nskeleton code on Canvas\nYour minimal shell will support:\n\nbasic command execution\nbuilt-in commands\nsignal handling\nI/O redirection\npiping"
  },
  {
    "objectID": "slosh.html#requirements",
    "href": "slosh.html#requirements",
    "title": "CSC 453",
    "section": "Requirements",
    "text": "Requirements\n\nC99 POSIX\nMust compile / run on the unix* servers\nYou must create a functional Makefile\nSubmit a tar (.tgz) file in Canvas"
  },
  {
    "objectID": "slosh.html#command-execution",
    "href": "slosh.html#command-execution",
    "title": "CSC 453",
    "section": "Command execution",
    "text": "Command execution\n\nExecute external commands\n\nUse fork(), exec(), and waitpid() to manage process execution\n\nSupport command-line arguments\nHandle both relative and absolute paths to executables\nReport command execution errors appropriately"
  },
  {
    "objectID": "slosh.html#redirection-piping",
    "href": "slosh.html#redirection-piping",
    "title": "CSC 453",
    "section": "Redirection / piping",
    "text": "Redirection / piping\n\nUse open(), dup2(), and close() for file redirection\nUse pipe() for creating pipes between processes"
  },
  {
    "objectID": "slosh.html#signal-handling",
    "href": "slosh.html#signal-handling",
    "title": "CSC 453",
    "section": "Signal handling",
    "text": "Signal handling\n\nSignals are async. They can happen at any time\nCertain functions are async-safe, others are not\n\nprintf() contains internal buffers and state; an interruption can lead to corrupted buffers"
  },
  {
    "objectID": "slosh.html#handling-sigint",
    "href": "slosh.html#handling-sigint",
    "title": "CSC 453",
    "section": "Handling SIGINT",
    "text": "Handling SIGINT\n\nwrite() a newline or whatever to signify the signal was caught.\nIf there is not a child running, write() a new prompt"
  },
  {
    "objectID": "slosh.html#parent-vs-child",
    "href": "slosh.html#parent-vs-child",
    "title": "CSC 453",
    "section": "Parent vs child",
    "text": "Parent vs child\n\nThe child should immediately reset signal handling to the default\nLook up signal()’s man\nman 2 signal"
  },
  {
    "objectID": "slosh.html#sigaction",
    "href": "slosh.html#sigaction",
    "title": "CSC 453",
    "section": "Sigaction",
    "text": "Sigaction\n\nsigaction() is newer than signal()\nOffers more control and is more consistent across systems\nAllows specifying flags\n\nSA_RESTART (man 7 signal)\n\nWithout it, syscalls return with EINTR error if they are interrupted by a signal\nWith it, most syscalls automatically restart\n\n\nIs the POSIX-compliant approach\nEINTR can still happen, it’s wise to check for it and handle it\n\nWhen reading input from user"
  },
  {
    "objectID": "slosh.html#section",
    "href": "slosh.html#section",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day7.html#admin",
    "href": "day7.html#admin",
    "title": "CSC 453",
    "section": "Admin",
    "text": "Admin\n\nBreak from programming assignments this week"
  },
  {
    "objectID": "day7.html#questions-to-consider",
    "href": "day7.html#questions-to-consider",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat is the difference between user threads and kernel threads?\nWhat are the different kernel thread models and how do they compare?\nHow do the different thread models trade off between scheduling control and parallelism?"
  },
  {
    "objectID": "day7.html#first-things-first",
    "href": "day7.html#first-things-first",
    "title": "CSC 453",
    "section": "First things first",
    "text": "First things first\n\nA kernel thread is the unit of execution that the OS can run on a CPU core\nA process can only make progress if at least one of its kernel threads is scheduled on a core\nCPUs run kernel threads, not processes"
  },
  {
    "objectID": "day7.html#posix-threads-pthreads",
    "href": "day7.html#posix-threads-pthreads",
    "title": "CSC 453",
    "section": "POSIX threads (pthreads)",
    "text": "POSIX threads (pthreads)\n\n60 calls, but the important ones are: pthread*\n\ncreate, exit, yield, join (Look familiar?)\n\nPthreads were historically (pre 2003) a user space library: all functionality happens in user space, and the kernel knew nothing about them\n\ncan run on kernels or CPUs with no thread support\norders of magnitude faster than a kernel trap (everything is a local function call)\ncustom schedulers"
  },
  {
    "objectID": "day7.html#pure-user-space-threads",
    "href": "day7.html#pure-user-space-threads",
    "title": "CSC 453",
    "section": "Pure user space threads",
    "text": "Pure user space threads\n\nTotally unknown to the OS\nUser-level threads must “borrow” a kernel thread to run\nWhat happens when a thread makes an I/O-bound system call?\n\nAll threads will block, because the process is blocked\nSometimes a thread blocking a process is unavoidable: page fault: more later\n\nWhere we typically want threads is where there is a lot of I/O blocking or system calls: the web server example"
  },
  {
    "objectID": "day7.html#kernel-threads",
    "href": "day7.html#kernel-threads",
    "title": "CSC 453",
    "section": "Kernel threads",
    "text": "Kernel threads\n\nThread libraries can also have kernel support (this is the common case today)\nThere is no user space runtime environment or thread table: all is managed within the kernel\nSupport for multiple cores\nAll thread interfaces are system calls handled by the kernel"
  },
  {
    "objectID": "day7.html#kernel-threads-contd",
    "href": "day7.html#kernel-threads-contd",
    "title": "CSC 453",
    "section": "Kernel threads (cont’d)",
    "text": "Kernel threads (cont’d)\n\nInstead of blocking on a system call, the kernel now has the intelligence to schedule another thread\nCreating and destroying threads comes at a higher cost (the kernel trap we were trying to avoid above)\n\nWhat should we do?\n\nThread pools: a collection of pre-allocated threads, assigned as needed"
  },
  {
    "objectID": "day7.html#thread-design-models-many-to-one-m1",
    "href": "day7.html#thread-design-models-many-to-one-m1",
    "title": "CSC 453",
    "section": "Thread design models: many-to-one (M:1)",
    "text": "Thread design models: many-to-one (M:1)\n\nKernel threads must support user level threads. There are multiple ways to pull this off\nMany-to-one\n\nA single kernel level thread supports many user level threads\nSwitching between threads is fast (done in user level)\nNo parallelism (kernel thread is supporting only a single user thread at a time)\nIf any user thread makes an I/O blocking call, all user threads in that process are blocked\n\nWhere does this make sense?\n\nIf you need complete control over scheduling\nIf you have no kernel or minimal RTOS kernel"
  },
  {
    "objectID": "day7.html#thread-models-many-to-many-mn",
    "href": "day7.html#thread-models-many-to-many-mn",
    "title": "CSC 453",
    "section": "Thread models: many-to-many (M:N)",
    "text": "Thread models: many-to-many (M:N)\n\nSet of user threads supported by &lt;= number of kernel\nUser thread isn’t bound to a particular kernel thread\n\nIf kernel thread has to block and was supporting 3 user threads, the others can migrate\n\nAllows parallelism\nProgrammer has more control (scheduling in user space)\nSounds like the best of all worlds, right?\n\nCan be hard to implement without decent language support\nNow you have two schedulers to deal with (user space and kernel)\ngoroutines are a notable example of M:M in reality (though they are achieved through the Go runtime and not the kernel itself)"
  },
  {
    "objectID": "day7.html#thread-models-one-to-one-11",
    "href": "day7.html#thread-models-one-to-one-11",
    "title": "CSC 453",
    "section": "Thread models: one-to-one (1:1)",
    "text": "Thread models: one-to-one (1:1)\n\nNumber of kernel-level threads is equal to the number of user-level threads (dominant model today)\nCan operate in parallel\nIf one blocks, others can continue\nDownsides?\n\nLose the advantage of fast switching between user threads\nAny new user thread requires kernel thread creation: expensive\nProgrammer loses scheduling control: kernel makes decisions\n\nVery popular model: Linux & Windows\n\nEasy to implement\nMost machines have multiple cores so we can support a lot of kernel threads"
  },
  {
    "objectID": "day7.html#which-models-make-sense",
    "href": "day7.html#which-models-make-sense",
    "title": "CSC 453",
    "section": "Which models make sense?",
    "text": "Which models make sense?\n\nDistributed scientific computing\n\n1:1 - In scientific computing, you want predictable, full-speed access to hardware, not an extra runtime scheduler in the way\n\nEmbedded system\n\nM:1 - In embedded systems, simplicity and determinism matter more than scalability. You may only have a single core\n\nNGINX web servers\n\nM:N - NGINX multiplexes connections (user-level work) onto a small number of kernel threads"
  },
  {
    "objectID": "day7.html#section",
    "href": "day7.html#section",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day7.html#questions-to-consider-1",
    "href": "day7.html#questions-to-consider-1",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are the requirements for a critical section synchronization solution?\nWhy do naive implementations like a simple boolean lock fail, and what must happen atomically instead?\nWhat are the practical limitations of Peterson’s solution, and why don’t they work well on modern architectures?"
  },
  {
    "objectID": "day7.html#where-do-we-stand",
    "href": "day7.html#where-do-we-stand",
    "title": "CSC 453",
    "section": "Where do we stand?",
    "text": "Where do we stand?\n\nProcess model allows us an abstraction of a program (and the state to pause it)\nScheduling allows us concurrency and marshaled access to a scarce resource: CPU\nMechanisms for process communication and cooperation (shared-memory or message passing)\nThreads allow concurrency within a process and shared memory\n\nWith shared memory, there is potential for inconsistency"
  },
  {
    "objectID": "day7.html#problem",
    "href": "day7.html#problem",
    "title": "CSC 453",
    "section": "Problem",
    "text": "Problem\n\nTwo processes accessing a reservation system, where the number of open seats is tracked by a counter\n\nA: Releasing Seat\nB: Reserving Seat\n\nCounter at 10; Process A counter++ = 11; Process B counter– = 9;\nWhat could go wrong?\nmov 0x8049a1c, %eax\nadd $0x1, %eax\nmov %eax, 0x8049a1c\n\nmov 0x8049a1c, %eax\nsub $0x1, %eax\nmov %eax, 0x8049a1c"
  },
  {
    "objectID": "day7.html#race-conditions",
    "href": "day7.html#race-conditions",
    "title": "CSC 453",
    "section": "Race conditions",
    "text": "Race conditions\n\nIf interleaved, whoever goes last wins. Either way, 11 or 9 is an incorrect value. This is a race condition\nVery common in multi-programming environments: allocating disk blocks or memory pages, writing to a file, network buffers, etc.\nHow can we solve race conditions?\n\nMutual Exclusion (one at a time) on Critical Sections (the contention data/code)"
  },
  {
    "objectID": "day7.html#challenges-we-face",
    "href": "day7.html#challenges-we-face",
    "title": "CSC 453",
    "section": "Challenges we face",
    "text": "Challenges we face\n\nHow large should a critical section be?\nHow long can a process be in a critical section\n\nWhat happens if they crash while within a critical section?\n\nHow do we implement? Who enforces entrance into critical sections?"
  },
  {
    "objectID": "day7.html#beware-naïve-implementations",
    "href": "day7.html#beware-naïve-implementations",
    "title": "CSC 453",
    "section": "Beware naïve implementations",
    "text": "Beware naïve implementations\n\nWhy not use a simple bool?\nbool lock = FALSE\ndo {\n  while (lock == TRUE);\n  lock = TRUE;\n  //CRITICAL SECTION\n  lock = FALSE;\n}while(TRUE);\nThis implementation requires TWO operations: a read (test) and a write (set) that must be atomic"
  },
  {
    "objectID": "day7.html#critical-section-solution-requirements",
    "href": "day7.html#critical-section-solution-requirements",
    "title": "CSC 453",
    "section": "Critical section solution requirements",
    "text": "Critical section solution requirements\n\nMutual exclusion: only one process can execute at a time\nProgress: if no process is executing in its critical section and there exist some processes that wish to enter their critical section, then the selection of the process that will enter the critical section next cannot be postponed indefinitely\n\nE.g., A process cannot immediately re-enter the critical section if the other process want to\n\nBounded waiting: In addition to guaranteeing entrance, limits on the amount of time in a critical section must be established\n\nNo assumptions on the speed or number of CPUs"
  },
  {
    "objectID": "day7.html#petersons-solution",
    "href": "day7.html#petersons-solution",
    "title": "CSC 453",
    "section": "Peterson’s solution",
    "text": "Peterson’s solution\n\nTwo processes: \\(P_i\\) and \\(P_j\\)\nTwo shared data items: int turn; bool flag[2];\ndo {\n  flag[i] = TRUE;\n  turn = j;\n  while (flag[j] && turn == j);\n  //CRITICAL SECTION\n  flag[i] = FALSE;\n  //REMAINDER\n}while(TRUE);\n\nturn indicates whose turn it is: turn == i → \\(P_i\\)’s turn\nif flag[i] == true → \\(P_i\\) is ready for the critical section\n\nTo enter: \\(P_j\\) sets flag[i] = true and turn = j\n\ni.e., \\(P_i\\) demurs to \\(P_j\\); if they are both ready turn will get set to?\n\nWhomever runs last sets the turn to the other process"
  },
  {
    "objectID": "day7.html#petersons-solution-contd",
    "href": "day7.html#petersons-solution-contd",
    "title": "CSC 453",
    "section": "Peterson’s solution (cont’d)",
    "text": "Peterson’s solution (cont’d)\ndo {\n  flag[i] = TRUE;\n  turn = j;\n  while (flag[j] && turn == j);\n  //CRITICAL SECTION\n  flag[i] = FALSE;\n  //REMAINDER\n}while(TRUE);\n\nDoes this meet the three solution requirements?\nMutual exclusion: under what conditions can a process be in the critical section?\n\n\\(P_i\\) only enters if flag[j] is false or turn == i\nConversely, if \\(P_i\\) and \\(P_j\\) are in the CS, then flag[i] == flag[j]; this implies both \\(P_i\\) and \\(P_j\\) terminated their whiles at the same time; BUT, in order for this to be true, turn must be both 0 and 1 (Contradiction)"
  },
  {
    "objectID": "day7.html#petersons-solution-contd-1",
    "href": "day7.html#petersons-solution-contd-1",
    "title": "CSC 453",
    "section": "Peterson’s solution (cont’d)",
    "text": "Peterson’s solution (cont’d)\ndo {\n  flag[i] = TRUE;\n  turn = j;\n  while (flag[j] && turn == j);\n  //CRITICAL SECTION\n  flag[i] = FALSE;\n  //REMAINDER\n}while(TRUE);\n\nProgress: Is there a condition where \\(P_i\\) can’t enter the CS?\n\nThere is no condition under which \\(P_i\\) cannot (eventually) enter the CS; either the other process will set turn == i or it is already set\n\nBounded Waiting: How long will \\(P_i\\) wait in the best/worst case?\n\n\\(P_i\\) will enter after at most one entry by \\(P_j\\)"
  },
  {
    "objectID": "day7.html#petersons-solution-contd-2",
    "href": "day7.html#petersons-solution-contd-2",
    "title": "CSC 453",
    "section": "Peterson’s solution (cont’d)",
    "text": "Peterson’s solution (cont’d)\ndo {\n  flag[i] = TRUE;\n  turn = j;\n  while (flag[j] && turn == j);\n  //CRITICAL SECTION\n  flag[i] = FALSE;\n  //REMAINDER\n}while(TRUE);\n\nDo you see any issues with this solution?\n\nLimitation: Only works for two processes\nModern architectures may also break this solution\n\nProcessors and/or compilers can reorder reads and writes that have no dependencies\nMultithreaded applications could lead to issues"
  },
  {
    "objectID": "day7.html#section-1",
    "href": "day7.html#section-1",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day7.html#questions-to-consider-2",
    "href": "day7.html#questions-to-consider-2",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nHow do synchronization solutions compare in implementation / function?\nWhy can busy-waiting primitives cause problems like priority inversion, and how do blocking primitives help address this?\nWhen would you choose a counting semaphore versus a binary semaphore or mutex, and what distinguishes them?"
  },
  {
    "objectID": "day7.html#hardware-solutions-atomicity",
    "href": "day7.html#hardware-solutions-atomicity",
    "title": "CSC 453",
    "section": "Hardware solutions: atomicity",
    "text": "Hardware solutions: atomicity\n\nOne way: use atomic instructions (all-or-none actions)\nBaked into modern hardware\n\ntest_and_set()\ncompare_and_swap()\nThese both lock the memory bus (lighter weight than the olden days when we could alternatively disable interrupts)\n\nDisabling interrupts only really works on a single CPU, modern architectures make it untenable\n\n\nEasy enough for simple operations\nNot realistic for complex operations\n\nEx. “atomic update of a B-tree”: do we really want to bake such an instruction into the hardware? No"
  },
  {
    "objectID": "day7.html#hardware-solutions-atomicity-contd",
    "href": "day7.html#hardware-solutions-atomicity-contd",
    "title": "CSC 453",
    "section": "Hardware solutions: atomicity (cont’d)",
    "text": "Hardware solutions: atomicity (cont’d)\nbool test_and_set(bool *lock){\n  bool ret = *lock;\n  *lock = TRUE;\n  return ret;\n}\n\nlock = FALSE;\ndo{\n  while(test_and_set(&lock));\n  // CRITICAL SECTION\n  lock = FALSE;\n}while(true)\n\nDoes this test_and_set() solution meet the requirements?\nMutual exclusion?\n\nYes\n\nBounded wait/Progress?\n\nNo: you could end up with a situation where an unlucky process is endlessly waiting based on the scheduler’s decisions"
  },
  {
    "objectID": "day7.html#mutex-locks",
    "href": "day7.html#mutex-locks",
    "title": "CSC 453",
    "section": "Mutex locks",
    "text": "Mutex locks\n\nPrevious solutions are complicated and generally not used by application programmers (kernel hackers only)\nAny solution to the critical section problem requires a lock\n\nPeterson’s solution is an example (in software)\ntest_and_set() / compare_and_swap() are hardware implementations\n\nSimplest commonly used solution is mutex lock\n\nBoolean variable indicating if lock is available or not"
  },
  {
    "objectID": "day7.html#mutex-locks-contd",
    "href": "day7.html#mutex-locks-contd",
    "title": "CSC 453",
    "section": "Mutex locks (cont’d)",
    "text": "Mutex locks (cont’d)\n\nProtect a critical section by\n\nFirst acquire() a lock\nThen release() the lock\n\nWhat’s going on under the hood?\nvoid lock(lock_t *lock) {\n  while (test_and_set(&lock-&gt;flag, 1) == 1)\n  ; // spin-wait (do nothing)\n}\n\ntest_and_set() or compare_and_swap()"
  },
  {
    "objectID": "day7.html#mutex-locks-contd-1",
    "href": "day7.html#mutex-locks-contd-1",
    "title": "CSC 453",
    "section": "Mutex locks (cont’d)",
    "text": "Mutex locks (cont’d)\n\nCalls to acquire() and release() must be atomic\nBut this solution requires busy waiting\n\nThis type of mutex is therefore often called a spinlock\n\nNOT ALWAYS TERRIBLE: spinlocks are preferable in some cases. When do you think?\n\nWhen the lock is likely to be held for &lt;= the time for two context switches\n\n\n\nThere are also mutex implementations (e.g., futex) that block to avoid busy-waiting, and some that do both\nNote: mutexes are owned by the thread that locks them. Only the thread that acquired the mutex can release it"
  },
  {
    "objectID": "day7.html#busy-waiting-consequence",
    "href": "day7.html#busy-waiting-consequence",
    "title": "CSC 453",
    "section": "Busy waiting consequence",
    "text": "Busy waiting consequence\nIn 1997, NASA’s Mars Pathfinder started having mysterious resets on Mars. From Earth, engineers saw the system repeatedly rebooting for no obvious reason. Not ideal when your computer is 250 million km away.\nThe cause turned out to be priority inversion in the operating system.\n\nWhat was happening:\n\nA low-priority task was holding a shared resource (a mutex).\n\nA high-priority task needed that resource and blocked, waiting.\nMeanwhile, a medium-priority task—which didn’t need the resource at all—kept preempting the low-priority task.\nResult: the low-priority task never got CPU time to release the resource, so the high-priority task was effectively starved.\n\n\nThe system interpreted this long delay as a fault and triggered a watchdog reset. Over and over."
  },
  {
    "objectID": "day7.html#priority-inversion",
    "href": "day7.html#priority-inversion",
    "title": "CSC 453",
    "section": "Priority inversion",
    "text": "Priority inversion\n\nAny ideas on how to fix?\n\nPriority inheritance: low priority processes are temporarily given priorities that match those waiting on them"
  },
  {
    "objectID": "day7.html#section-2",
    "href": "day7.html#section-2",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day7.html#semaphores",
    "href": "day7.html#semaphores",
    "title": "CSC 453",
    "section": "Semaphores",
    "text": "Semaphores\n\n\n\nSynchronization tool that provides more sophisticated ways (than mutex locks) for processes to synchronize their activities\nSemaphore S: integer variable\nCan only be accessed via two atomic operations\n\nwait() and signal()\n\nOriginally called P() and V() (because, Dutch - another Dijkstra invention)\n\n\nDo not have ownership, any thread can signal (increment) or wait (decrement)\n\n\n\n\nwait(S) { \n    while (S &lt;= 0)\n       ; // busy\n    S--;\n}\n\nsignal(S) { \n    S++;\n}"
  },
  {
    "objectID": "day7.html#semaphores-contd",
    "href": "day7.html#semaphores-contd",
    "title": "CSC 453",
    "section": "Semaphores (cont’d)",
    "text": "Semaphores (cont’d)\n\n\n\nBinary semaphore: integer value can range only between 0 and 1\n\nFunctionally the same as a mutex (often when people call something a mutex it’s actually a binary semaphore)\n\n\n\n\n\ndo{\n  wait(mutex);\n  //CS\n  signal(mutex);\n}while(TRUE);\n\n\nCounting semaphore: integer value can range over an unrestricted domain\n\nWhen would this make sense?\n\nProtect a finite set of resources rather than a single one (e.g., you have a pool of db connections that can be shared by processes)\nInitialize S to the number of resources\nDecrement S each time someone grabs a resource, increment when they release"
  },
  {
    "objectID": "day7.html#semaphores-contd-1",
    "href": "day7.html#semaphores-contd-1",
    "title": "CSC 453",
    "section": "Semaphores (cont’d)",
    "text": "Semaphores (cont’d)\n\n\n\nCan also be used to force synchronization\nConsider \\(P_1\\) and \\(P_2\\) that with two statements \\(S_1\\) and \\(S_2\\) and the requirement that \\(S_1\\) to happen before \\(S_2\\)\n\n\n\n\nP1:\n   S1;\n   signal(sync);\nP2:\n   wait(sync);\n   S2;"
  },
  {
    "objectID": "day7.html#blocking-semaphores",
    "href": "day7.html#blocking-semaphores",
    "title": "CSC 453",
    "section": "Blocking semaphores",
    "text": "Blocking semaphores\n\n\n\nWe add sleep() and wakeup() to the underlying implementation\nCall to wait, and semaphore not available, process is blocked with sleep() (wait state) and placed on a waiting queue\nBlocked processes are notified of an available semaphore by the wakeup() operation\n\ngoes from waiting to ready\nAble to achieve bounded wait and progress with FIFO queue\n\n\n\n\n\nwait(semaphore *S) { \n   S-&gt;value--; \n   if (S-&gt;value &lt; 0) {      \n      add process to S-&gt;list; \n      sleep(); \n   } \n}\n\nsignal(semaphore *S) { \n   S-&gt;value++; \n   if (S-&gt;value &lt;= 0) {      \n      remove P from S-&gt;list; \n      wakeup(P); \n   } \n}"
  },
  {
    "objectID": "day7.html#mutexes-vs.-semaphores",
    "href": "day7.html#mutexes-vs.-semaphores",
    "title": "CSC 453",
    "section": "Mutexes vs. semaphores",
    "text": "Mutexes vs. semaphores\n\nMutexes are generally lighter weight / faster\nSemaphores can support multiple instances\nSemaphores don’t have the ownership limitations"
  },
  {
    "objectID": "day7.html#choose-your-primitive",
    "href": "day7.html#choose-your-primitive",
    "title": "CSC 453",
    "section": "Choose your primitive",
    "text": "Choose your primitive\n\nFor each, choose between: atomic instructions; futexes; spin lock mutexes; semaphores\nScenario 1: You need to protect a critical section where only one thread should execute at a time, and the critical section is expected to be held for a relatively long duration.\n\nFutex\n\nScenario 2: You have a pool of database connections, and you need to limit the number of threads that can access these connections simultaneously.\n\nCounting semaphore"
  },
  {
    "objectID": "day7.html#choose-your-primitive-contd",
    "href": "day7.html#choose-your-primitive-contd",
    "title": "CSC 453",
    "section": "Choose your primitive (cont’d)",
    "text": "Choose your primitive (cont’d)\n\nFor each, choose between: atomic instructions; futexes; spin lock mutexes; semaphores\nScenario 3: You need to increment a shared counter frequently, and the operation is very quick.\n\nAtomic instruction\n\nScenario 4: You need to protect a critical section where only one thread should execute at a time, but the critical section is expected to be held for a very short duration.\n\nSpin lock mutex"
  },
  {
    "objectID": "day7.html#beware-mistakes-are-easy-to-make",
    "href": "day7.html#beware-mistakes-are-easy-to-make",
    "title": "CSC 453",
    "section": "Beware: mistakes are easy to make",
    "text": "Beware: mistakes are easy to make\n\nIncorrect use of semaphore operations\n\nwrong order: signal(mutex) … wait(mutex)\nrepeating: wait(mutex) … wait(mutex)\nOmitting of wait(mutex) and/or signal(mutex)\n\nBe careful"
  },
  {
    "objectID": "day7.html#monitors",
    "href": "day7.html#monitors",
    "title": "CSC 453",
    "section": "Monitors",
    "text": "Monitors\n\nSubtle mistakes with semaphores can lead to deadlocks, leading to monitors\nCan think of monitors as a library with an API (abstract data type)\n\nProcesses share the library, but not internal data (directly)\nThis requires language specific understanding of a monitor (C doesn’t have them, natively)\nMore relevant in languages like Java and other high-level languages that provide built-in monitor support"
  },
  {
    "objectID": "day7.html#monitors-contd",
    "href": "day7.html#monitors-contd",
    "title": "CSC 453",
    "section": "Monitors (cont’d)",
    "text": "Monitors (cont’d)\n\n\n\nMain idea: only one processes can be active within a monitor at any instant\n\nUp to the compilers to ensure mutual exclusion on monitor procedures\n\nIt can use other sync primitives to achieve this: e.g. a semaphore or mutex\n\nWe’re offloading synchronization correctness to the compiler, (hopefully) lessening the chance for error by the user\n\nMonitor variables are private"
  },
  {
    "objectID": "day7.html#condition-variables",
    "href": "day7.html#condition-variables",
    "title": "CSC 453",
    "section": "Condition variables",
    "text": "Condition variables\n\nMonitors often have condition variables\nCondition variables have wait() and signal() operations\n\nx.wait(): a process that invokes the operation is suspended until x.signal()\nx.signal(): resumes one of processes (if any) that invoked x.wait()\n\nIf no x.wait() on the variable, then it has no effect on the variable\n\n\nBig benefit of condition variables: x.broadcast()"
  },
  {
    "objectID": "day7.html#mutexes-semaphores-monitors-condition-variables",
    "href": "day7.html#mutexes-semaphores-monitors-condition-variables",
    "title": "CSC 453",
    "section": "Mutexes, semaphores, Monitors, Condition variables",
    "text": "Mutexes, semaphores, Monitors, Condition variables\n\nMutexes and Semaphores are good for:\n\nSimple mutual exclusion\nSimple counting resources\n\nMonitors are good for:\n\nComplex synchronization between many threads\nThread-safe operations (only allow one thread into the monitor at a time)\n\nCondition variables are good for:\n\nProducer-consumer problems (consumer threads waiting on some condition, signaled when ready)\nWe want to make one or more threads sleep until a resource is ready"
  },
  {
    "objectID": "day7.html#section-3",
    "href": "day7.html#section-3",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day2.html#admin",
    "href": "day2.html#admin",
    "title": "CSC 453",
    "section": "Admin",
    "text": "Admin\n\nQuiz 1 due Friday\n\nNOTE: this first quiz allows unlimited attempts. The rest will not.\n\nLab 1 due Monday\nAssignment 1 due Jan 19\n\nI’ll talk a bit about this during lab today\n\nI realize this seem like a lot, but time is short"
  },
  {
    "objectID": "day2.html#questions-to-consider",
    "href": "day2.html#questions-to-consider",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are the main abstractions OSes provide?\nWhat are the abstraction challenges?"
  },
  {
    "objectID": "day2.html#abstractions",
    "href": "day2.html#abstractions",
    "title": "CSC 453",
    "section": "Abstractions",
    "text": "Abstractions\n\nAbstractions simplify application design by:\n\nhiding undesirable properties,\nadding new capabilities, and\norganizing information\n\nAbstractions provide an interface to application programmers that separates policy—what the interface commits to accomplishing—from mechanism—how the interface is implemented."
  },
  {
    "objectID": "day2.html#what-are-the-abstractions",
    "href": "day2.html#what-are-the-abstractions",
    "title": "CSC 453",
    "section": "What are the abstractions?",
    "text": "What are the abstractions?\n\nCPUs\n\nProcesses, threads\n\nMemory\n\nAddress space\n\nDisk\n\nFiles"
  },
  {
    "objectID": "day2.html#example-os-abstraction-file-systems",
    "href": "day2.html#example-os-abstraction-file-systems",
    "title": "CSC 453",
    "section": "Example OS abstraction: file systems",
    "text": "Example OS abstraction: file systems\n\nWhat undesirable properties do file systems hide?\n\nDisks are slow!\nChunks of storage are actually distributed all over the disk\nDisk storage may fail!\n\nWhat new capabilities do files add?\n\nGrowth and shrinking\nOrganization into directories, searchability\n\nWhat information do files help organize?\n\nOwnership and permissions\nAccess time, modification time, type, etc."
  },
  {
    "objectID": "day2.html#abstraction-tradeoffs---discussion",
    "href": "day2.html#abstraction-tradeoffs---discussion",
    "title": "CSC 453",
    "section": "Abstraction tradeoffs - discussion",
    "text": "Abstraction tradeoffs - discussion\n\nIdentify undesirable properties hidden by, new capabilities added, and info organization provided with these abstractions:\n\nProcess / threads\nAddress space\n\n\n\nProcesses/Threads\n\nHiding undesirable properties:\n\nExample: The process abstraction hides the complexity of CPU scheduling and context switching. Applications don’t need to manage the low-level details of how the CPU switches between different tasks. Similarly, the thread abstraction hides the intricacies of managing multiple execution paths within a single process, allowing developers to focus on the logic of concurrent tasks.\n\nAdding new capabilities:\n\nExample: Processes provide isolation between different applications, ensuring that one misbehaving application doesn’t affect others. Threads within a process allow for parallel execution of tasks, improving performance and responsiveness. Additionally, operating systems often provide inter-process communication (IPC) mechanisms, enabling processes to coordinate and share data.\n\nOrganizing information:\n\nExample: The process abstraction organizes the execution environment by encapsulating the code, data, and resources needed for a program to run. Threads further organize execution by dividing tasks within a process into smaller, manageable units. This organization helps in structuring complex applications and improving their maintainability.\n\n\nMemory Address Space\n\nHiding undesirable properties:\n\nExample: The memory address space abstraction hides the physical memory layout from applications. Programs use virtual addresses, which the operating system maps to physical memory locations. This abstraction also hides the details of memory fragmentation and allocation, simplifying memory management for developers.\n\nAdding new capabilities:\n\nExample: Virtual memory allows applications to use more memory than physically available by swapping data to and from disk. This capability enables larger and more complex applications to run on systems with limited physical memory. Additionally, memory protection mechanisms prevent one process from accessing the memory of another, enhancing system stability and security.\n\nOrganizing information:\n\nExample: The memory address space abstraction organizes memory into segments such as code, data, heap, and stack. This organization helps in managing different types of data efficiently and ensures that memory is used in a structured manner. For instance, the stack is used for function calls and local variables, while the heap is used for dynamic memory allocation."
  },
  {
    "objectID": "day2.html#abstraction-pros-cons",
    "href": "day2.html#abstraction-pros-cons",
    "title": "CSC 453",
    "section": "Abstraction pros / cons",
    "text": "Abstraction pros / cons\n\nAdvantages of OS providing abstractions?\n\nAllow applications to reuse common facilities\nMake different devices look the same\nProvide higher-level or more useful functionality\n\nChallenges?\n\nWhat are the correct abstractions?\nHow much should be exposed?"
  },
  {
    "objectID": "day2.html#os-design-requirements---what-do-we-need",
    "href": "day2.html#os-design-requirements---what-do-we-need",
    "title": "CSC 453",
    "section": "OS design requirements - what do we need?",
    "text": "OS design requirements - what do we need?\n\nConvenience, abstraction of hardware resources for user programs\nEfficiency of usage of CPU, memory, etc.\nIsolation between multiple processes\nReliability, the OS must not fail\nOther:\n\nSecurity\nMobility"
  },
  {
    "objectID": "day2.html#section",
    "href": "day2.html#section",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day2.html#questions-to-consider-1",
    "href": "day2.html#questions-to-consider-1",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nHow does the OS manage access to resources?"
  },
  {
    "objectID": "day2.html#os-as-a-resource-manager",
    "href": "day2.html#os-as-a-resource-manager",
    "title": "CSC 453",
    "section": "OS as a resource manager",
    "text": "OS as a resource manager\n\nAnother view: resource manager - shares resources “well”\nAdvantages of the OS managing resources?\n\nProtect applications from one another\nProvide efficient access to resources (cost, time, energy)\nProvide fair access to resources\n\nChallenges?\n\nWhat are the correct mechanisms?\nWhat are the correct policies?"
  },
  {
    "objectID": "day2.html#resources-are-managed-via-services",
    "href": "day2.html#resources-are-managed-via-services",
    "title": "CSC 453",
    "section": "Resources are managed via services",
    "text": "Resources are managed via services\n\nProgram Execution (loading, running, monitoring, terminating)\nPerformance (optimizing resources under constraints)\nCorrectness (overseeing critical operations, preventing interference)\nFairness (access to and allocation of resources)\nError detection & recovery (network partition & media failure)"
  },
  {
    "objectID": "day2.html#services-contd",
    "href": "day2.html#services-contd",
    "title": "CSC 453",
    "section": "Services (cont’d)",
    "text": "Services (cont’d)\n\nCommunication (inter-process, software-to-hardware, hardware-to-hardware, system-to-system, wide-area)\nI/O: reading & writing, support for various mediums, devices, performance, and protections\nData Organization (naming), Services (search) & Protection (access control)\nSecurity (isolation, enforcement, services, authentication, accounting and logging, trust)\nUser interfaces (command-line, GUIs, multiple users)"
  },
  {
    "objectID": "day2.html#each-service-has-challenges-and-tensions",
    "href": "day2.html#each-service-has-challenges-and-tensions",
    "title": "CSC 453",
    "section": "Each service has challenges and tensions",
    "text": "Each service has challenges and tensions\nExample 1: We have limited RAM, and we want to run more programs that can be stored.\n\nHow do we allocate space?\nWho stays?\nWho goes?\nWhat if we’re wrong?\nWhat if the system is under extremely heavy load?\nIs there a way to predict the future?"
  },
  {
    "objectID": "day2.html#each-service-has-challenges-and-tensions-1",
    "href": "day2.html#each-service-has-challenges-and-tensions-1",
    "title": "CSC 453",
    "section": "Each service has challenges and tensions",
    "text": "Each service has challenges and tensions\nExample 2: We have two process (producer / consumer); how do they communicate?\n\nMessage passing? Shared memory?\nHow do they synchronize?\n\nHow to we prevent over-production? Over-consumption?\nContext-switching?"
  },
  {
    "objectID": "day2.html#section-1",
    "href": "day2.html#section-1",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day2.html#questions-to-consider-2",
    "href": "day2.html#questions-to-consider-2",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat is the chain of events that takes place before you are able to run a process?\nWhere do the different parts of the chain reside? How are they called?"
  },
  {
    "objectID": "day2.html#boot-is-like-a-relay-race",
    "href": "day2.html#boot-is-like-a-relay-race",
    "title": "CSC 453",
    "section": "Boot is like a relay race",
    "text": "Boot is like a relay race\n\nA sequence of programs\nEach stage:\n\nHas slightly more power\nLoads the next stage\nJumps to it"
  },
  {
    "objectID": "day2.html#power-on",
    "href": "day2.html#power-on",
    "title": "CSC 453",
    "section": "Power-On",
    "text": "Power-On\n\nCPU resets\nInstruction pointer set to a fixed address\nExecution begins in firmware (ROM)\nThe CPU does not know what an OS is"
  },
  {
    "objectID": "day2.html#firmware-bios-uefi",
    "href": "day2.html#firmware-bios-uefi",
    "title": "CSC 453",
    "section": "Firmware (BIOS / UEFI)",
    "text": "Firmware (BIOS / UEFI)\n\nInitializes RAM\nInitializes minimal hardware\nFinds one program to run (bootloader)\nLoads it into memory\nJumps to it"
  },
  {
    "objectID": "day2.html#bootloaders-job-grub-example",
    "href": "day2.html#bootloaders-job-grub-example",
    "title": "CSC 453",
    "section": "Bootloader’s job (GRUB Example)",
    "text": "Bootloader’s job (GRUB Example)\n\nUnderstand filesystems\nLoad the kernel image\nLoad the initramfs\nPass arguments to the kernel\nJump to the kernel entry point\nProvides kernel command line args\nroot=/dev/sda1\ninit=/sbin/init\nconsole=ttyS0"
  },
  {
    "objectID": "day2.html#what-the-bootloader-does-not-do",
    "href": "day2.html#what-the-bootloader-does-not-do",
    "title": "CSC 453",
    "section": "What the bootloader does not do",
    "text": "What the bootloader does not do\n\nDoes not schedule processes\nDoes not manage memory long-term\nDoes not run the OS"
  },
  {
    "objectID": "day2.html#kernel",
    "href": "day2.html#kernel",
    "title": "CSC 453",
    "section": "Kernel",
    "text": "Kernel\nThink of the kernel in phases\n\nSelf-setup\nHardware abstraction\nUserspace handoff"
  },
  {
    "objectID": "day2.html#kernel-self-setup",
    "href": "day2.html#kernel-self-setup",
    "title": "CSC 453",
    "section": "Kernel self-setup",
    "text": "Kernel self-setup\n\nSets up page tables\nEnables virtual memory\nInitializes scheduler\nSets up interrupts\n\n\nThe kernel cannot rely on anything yet: it is building the world."
  },
  {
    "objectID": "day2.html#kernel-hardware-abstractions",
    "href": "day2.html#kernel-hardware-abstractions",
    "title": "CSC 453",
    "section": "Kernel hardware abstractions",
    "text": "Kernel hardware abstractions\nThis is where hardware becomes an abstraction.\n\nDetects hardware\nLoads drivers\nTurns devices into files:\n\n/dev/sda\n/dev/tty\n\nCreates kernel threads"
  },
  {
    "objectID": "day2.html#userspace-handoff",
    "href": "day2.html#userspace-handoff",
    "title": "CSC 453",
    "section": "Userspace handoff",
    "text": "Userspace handoff\n\nMounts the root filesystem\nChooses one program to run\nExecutes it with execve()\nexecve(\"/sbin/init\", ...);"
  },
  {
    "objectID": "day2.html#init-pid-1",
    "href": "day2.html#init-pid-1",
    "title": "CSC 453",
    "section": "Init (PID 1)",
    "text": "Init (PID 1)\n\nFirst userspace process\nAlways PID 1\nParent of all other processes\n\n\nIf PID 1 exits?\n\nThe kernel panics or shuts down"
  },
  {
    "objectID": "day2.html#section-2",
    "href": "day2.html#section-2",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day2.html#questions-to-consider-3",
    "href": "day2.html#questions-to-consider-3",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are the different kernel paradigms?\nWhy would you choose one over the other?"
  },
  {
    "objectID": "day2.html#paradigms",
    "href": "day2.html#paradigms",
    "title": "CSC 453",
    "section": "Paradigms",
    "text": "Paradigms\n\nThe OS offers a number of services. What should go in the kernel?\n\nIPC\nVFS\nFile system\nScheduler\nVirtual Memory\nDevice drivers"
  },
  {
    "objectID": "day2.html#monolithic-kernels",
    "href": "day2.html#monolithic-kernels",
    "title": "CSC 453",
    "section": "Monolithic kernels",
    "text": "Monolithic kernels\n\nOldest, very common design (Linux, Windows 9x, BSDs)\nSingle piece of code in memory\nLimited information hiding\n\nOne part of the kernel can directly access data and functions of other parts"
  },
  {
    "objectID": "day2.html#monolithic-kernels-contd",
    "href": "day2.html#monolithic-kernels-contd",
    "title": "CSC 453",
    "section": "Monolithic kernels (cont’d)",
    "text": "Monolithic kernels (cont’d)\n\nQ: What happens when you need something new supported (new hardware device, new system call, new filesystem)?\nModules (loadable kernel modules - LKMs) allow for flexibility, customization, support\n\nPros?\n\nMemory savings\nFlexibility\nMinimal downtime\n\nCons?\n\n(minor) fragmentation: the base kernel can be loaded contiguously in memory\nSecurity (https://github.com/m0nad/Diamorphine DO NOT USE THIS, IT’S SIMPLY TO SHOW YOU)"
  },
  {
    "objectID": "day2.html#layered-kernels",
    "href": "day2.html#layered-kernels",
    "title": "CSC 453",
    "section": "Layered kernels",
    "text": "Layered kernels\n\nDijkstra created the THE OS in the 60s, introducing the concept Each inner layer is more privileged; required a trap to move down layers\nHardware-enforcement possible\nIntel announced in May 2024 that the new architectures will only have rings 0 and 3"
  },
  {
    "objectID": "day2.html#microkernels",
    "href": "day2.html#microkernels",
    "title": "CSC 453",
    "section": "Microkernels",
    "text": "Microkernels\n\n\n\nPopular research area long ago, didn’t win (although people remain interested in the principles)\nAll non-essential components removed from the kernel, for modularization, extensibility, isolation, security, and ease of management\nA collection of OS services running in user space\nDownsides?\n\nHeavy communication costs through message passing (marshaled through the kernel)"
  },
  {
    "objectID": "day2.html#monolithic-vs-microkernel",
    "href": "day2.html#monolithic-vs-microkernel",
    "title": "CSC 453",
    "section": "Monolithic vs Microkernel",
    "text": "Monolithic vs Microkernel"
  },
  {
    "objectID": "day2.html#section-3",
    "href": "day2.html#section-3",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day6.html#admin",
    "href": "day6.html#admin",
    "title": "CSC 453",
    "section": "Admin",
    "text": "Admin\n\nMidterm 1 Thursday\n\nReminder: all raw slide content is in the GitHub repo as *.qmd\n\nNo quiz this week\nLab 4 due Monday\nProgramming assignment 2 due Monday"
  },
  {
    "objectID": "day6.html#questions-to-consider",
    "href": "day6.html#questions-to-consider",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are the key differences between batch and interactive scheduling algorithms?\nHow does Round Robin improve response time compared to SJF, and what tradeoff does it introduce?\nWhy might a system benefit from using multiple scheduling queues rather than a single algorithm?"
  },
  {
    "objectID": "day6.html#batch-vs.-interactive-scheduling",
    "href": "day6.html#batch-vs.-interactive-scheduling",
    "title": "CSC 453",
    "section": "Batch vs. interactive scheduling",
    "text": "Batch vs. interactive scheduling\n\n\nBatch Scheduling\n\nOptimizes for: throughput, turnaround time, waiting time\nContext: data centers, HPC, background jobs\nUsers don’t interact with running jobs\n\n\n\n\nInteractive Scheduling\n\nOptimizes for: response time, fairness\nContext: desktop/laptop systems, servers\nUsers expect quick feedback to their actions"
  },
  {
    "objectID": "day6.html#what-about-response-time",
    "href": "day6.html#what-about-response-time",
    "title": "CSC 453",
    "section": "What about response time?",
    "text": "What about response time?\n\n\n\nSRTF is good if we know job lengths and we’re only worried about turnaround / waiting time\nSJF: poor response\nRR: better response"
  },
  {
    "objectID": "day6.html#interactive-algorithm-round-robin-rr",
    "href": "day6.html#interactive-algorithm-round-robin-rr",
    "title": "CSC 453",
    "section": "Interactive algorithm: Round Robin (RR)",
    "text": "Interactive algorithm: Round Robin (RR)\n\nFCFS with preemption\nBased on a time quantum (time slice)\n\nTypically 10-100 milliseconds\nHow do we pick a quantum?\n\nVery short: a lot of context switching (need a large quantum, otherwise overhead is too much to be worth it)\nVery long -&gt; becomes FCFS: long wait times, long turnaround times\n\nCircular queue\n\nNew processes are added to the tail\nIf a process doesn’t complete during its quantum, context switch and added to the tail"
  },
  {
    "objectID": "day6.html#rr-example",
    "href": "day6.html#rr-example",
    "title": "CSC 453",
    "section": "RR example",
    "text": "RR example\n\n\n\nAverage waiting time:  \\[[(10-4) + 4 + 7] = 17/3 = 5.66\\]\nPros:\n\nRR is fair, but do we really want fair???\nTypically, higher average turnaround than SJF, but better response\n\nCons?\n\nLong average waiting times\n\n\n\n\n\nProcess Burst Time\n  P1        24\n  P2        3\n  P3        3   \n\n  Quantum = 4"
  },
  {
    "objectID": "day6.html#gaming-rr",
    "href": "day6.html#gaming-rr",
    "title": "CSC 453",
    "section": "Gaming RR",
    "text": "Gaming RR\n\nHow could a programmer “cheat” RR?\n\nSpin up many processes (split tasks up and fork children)\nJob A: 9 processes\nJob B: 1 process\nA gets 90% of the CPU in RR\n\nThis actually happens in many systems that aim for rudimentary fairness"
  },
  {
    "objectID": "day6.html#fcfs-sjf-srtf-rr",
    "href": "day6.html#fcfs-sjf-srtf-rr",
    "title": "CSC 453",
    "section": "FCFS, SJF, SRTF, RR",
    "text": "FCFS, SJF, SRTF, RR\n\nAll have downsides\nThose that optimize turnaround / wait, can harm response time\nThose that optimize response time, can harm turnaround, wait\nWhat should we do?"
  },
  {
    "objectID": "day6.html#interactive-algorithms-multi-level-queue-mlq",
    "href": "day6.html#interactive-algorithms-multi-level-queue-mlq",
    "title": "CSC 453",
    "section": "Interactive algorithms: Multi-Level Queue (MLQ)",
    "text": "Interactive algorithms: Multi-Level Queue (MLQ)\n\nWe have classes of process, with different needs. Why not multiple schedulers satisfying those needs?\nMultilevel queue scheduler defined by the following parameters:\n\n# of queues\nScheduling algorithms for each queue\nMethod used to determine which queue a process will enter when that process needs service\nScheduling among the queues"
  },
  {
    "objectID": "day6.html#mlq",
    "href": "day6.html#mlq",
    "title": "CSC 453",
    "section": "MLQ",
    "text": "MLQ\n\n\n\nHow do we schedule this?\n\nStrict priority: low priority could starve\nTypical: time slice among queues (e.g., real-time get 50%, system gets 30%, interactive 15%, batch 5%)\nEach queue can implement a separate scheduling policy\n\nMLQ cons:\n\nStarvation\nInflexibility"
  },
  {
    "objectID": "day6.html#interactive-algorithms-multi-level-feedback-queue-mlfq",
    "href": "day6.html#interactive-algorithms-multi-level-feedback-queue-mlfq",
    "title": "CSC 453",
    "section": "Interactive algorithms: Multi-Level Feedback Queue (MLFQ)",
    "text": "Interactive algorithms: Multi-Level Feedback Queue (MLFQ)\n\nA process can move between the various queues (goal is to avoid starvation present in MLQ)\nMetrics for movement:\n\nProcess requirements (we serve fast processes quickly)\nOver consumption\nChange in priority\nAge"
  },
  {
    "objectID": "day6.html#mlfq-rules",
    "href": "day6.html#mlfq-rules",
    "title": "CSC 453",
    "section": "MLFQ rules",
    "text": "MLFQ rules\n\nRule 1: If Priority(A) &gt; Priority(B), A runs (B doesn’t)\nRule 2: If Priority(A)=Priority(B), A & B run in RR\nRule 3: When a job enters the system, it is placed at the highest priority (the topmost queue)\nRule 4a: If a job uses up its allotment while running, its priority is reduced (i.e., it moves down one queue)\nRule 4b: If a job gives up the CPU (for example, by performing an I/O operation) before the allotment is up, it stays at the same priority level (i.e., its allotment is reset)\nAny problems with these???"
  },
  {
    "objectID": "day6.html#mlfq-example-one-process",
    "href": "day6.html#mlfq-example-one-process",
    "title": "CSC 453",
    "section": "MLFQ example: one process",
    "text": "MLFQ example: one process"
  },
  {
    "objectID": "day6.html#mlfq-example-two-processes",
    "href": "day6.html#mlfq-example-two-processes",
    "title": "CSC 453",
    "section": "MLFQ example: two processes",
    "text": "MLFQ example: two processes\n\n\n\nScenario 1: Perfectly fine"
  },
  {
    "objectID": "day6.html#mlfq-example-two-processes-1",
    "href": "day6.html#mlfq-example-two-processes-1",
    "title": "CSC 453",
    "section": "MLFQ example: two processes",
    "text": "MLFQ example: two processes\n\n\n\nScenario 1: Perfectly fine\nScenario 2: programmer can game the algorithm to remain at high priority"
  },
  {
    "objectID": "day6.html#mlfq-example-two-processes-2",
    "href": "day6.html#mlfq-example-two-processes-2",
    "title": "CSC 453",
    "section": "MLFQ example: two processes",
    "text": "MLFQ example: two processes\n\n\n\nScenario 1: Perfectly fine\nScenario 2: programmer can game the algorithm to remain at high priority\nScenario 3: multiple small processes can lead to starvation"
  },
  {
    "objectID": "day6.html#mlfq-how-can-we-solve-gaming",
    "href": "day6.html#mlfq-how-can-we-solve-gaming",
    "title": "CSC 453",
    "section": "MLFQ: how can we solve gaming?",
    "text": "MLFQ: how can we solve gaming?"
  },
  {
    "objectID": "day6.html#mlfq-solving-gaming",
    "href": "day6.html#mlfq-solving-gaming",
    "title": "CSC 453",
    "section": "MLFQ: solving gaming",
    "text": "MLFQ: solving gaming\n\n\n\nRule 4 (new): Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue)"
  },
  {
    "objectID": "day6.html#mlfq-how-can-we-avoid-starvation",
    "href": "day6.html#mlfq-how-can-we-avoid-starvation",
    "title": "CSC 453",
    "section": "MLFQ: how can we avoid starvation?",
    "text": "MLFQ: how can we avoid starvation?"
  },
  {
    "objectID": "day6.html#mlfq-solving-starvation",
    "href": "day6.html#mlfq-solving-starvation",
    "title": "CSC 453",
    "section": "MLFQ: solving starvation",
    "text": "MLFQ: solving starvation\n\n\n\nRule 5: After some time period, move all the jobs in the system to the topmost queue\n\nSolves starvation\nBonus: if a long running process evolves to be more interactive, it gets promoted"
  },
  {
    "objectID": "day6.html#what-scheduler-should-we-use",
    "href": "day6.html#what-scheduler-should-we-use",
    "title": "CSC 453",
    "section": "What scheduler should we use?",
    "text": "What scheduler should we use?\n\nTicket booking system\n\nFCFS: fairness\n\nPrint server\n\nSJF could make sense\n\nWeb server\n\nRR: fairness and short response time\n\nGeneral operating systems\n\nMLFQ: need to handle a mix"
  },
  {
    "objectID": "day6.html#linux-scheduling",
    "href": "day6.html#linux-scheduling",
    "title": "CSC 453",
    "section": "Linux scheduling",
    "text": "Linux scheduling\n\nCompletely Fair Scheduler (CFS)\nFair-share scheduling (see lottery and stride scheduling in the text if you want to learn more)\nBased on vruntime (how long any process has run): lowest vruntime runs next\nDivide sched_latency by # processes to determine a dynamic quantum (floor set by min_granularity)\n\nWhy do we need a floor?"
  },
  {
    "objectID": "day6.html#linux-cfs",
    "href": "day6.html#linux-cfs",
    "title": "CSC 453",
    "section": "Linux CFS",
    "text": "Linux CFS\n\nRed-Black tree includes only running (or ready) processes\nBalanced such that operations are \\(O(log n)\\)\nVery straightforward to find the next job (smallest vruntime) to run\nSleeper fairness: processes waiting on I/O are given “credit” for their wait time\nPeople are actively working on new features"
  },
  {
    "objectID": "day6.html#section",
    "href": "day6.html#section",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day6.html#questions-to-consider-1",
    "href": "day6.html#questions-to-consider-1",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are the differences between processes and threads?\nWhen are threads beneficial?\nHow are threads limited by Amdahl’s Law?"
  },
  {
    "objectID": "day6.html#motivation-context-switching-is-expensive",
    "href": "day6.html#motivation-context-switching-is-expensive",
    "title": "CSC 453",
    "section": "Motivation: context switching is expensive",
    "text": "Motivation: context switching is expensive\n\nconsider the modern word processor, which concurrently:\n\naccepts user input,\nperforms auto-correct/spell checking;\nauto-detects formatting\nauto-save (nothing could be entered until save was complete);\n\nAll impossible (or at least very inconvenient) without threads."
  },
  {
    "objectID": "day6.html#processes-vs-threads",
    "href": "day6.html#processes-vs-threads",
    "title": "CSC 453",
    "section": "Processes vs threads",
    "text": "Processes vs threads\n\nPotentially confusing due to overlapping terminology: we can describe both a process and a thread as running\nTerminology can be helpful for remembering the distinction:\n\nA computing process requires multiple resources: the CPU, memory, files, etc.\nA thread of execution abstracts CPU state"
  },
  {
    "objectID": "day6.html#processes-vs-threads-contd",
    "href": "day6.html#processes-vs-threads-contd",
    "title": "CSC 453",
    "section": "Processes vs threads (cont’d)",
    "text": "Processes vs threads (cont’d)\n\nProcesses contain threads; threads belong to a process\n\nOnly one exception: the kernel may have threads of execution not associated with any user process\n\nThreads each have their own stack and registers\n\nShare code, data, files held by the process\n\nA process is considered to be running when one or more of its threads are running\nDifferent operating systems use different terminology, but share common ideas"
  },
  {
    "objectID": "day6.html#intra-process-communication",
    "href": "day6.html#intra-process-communication",
    "title": "CSC 453",
    "section": "Intra-process communication",
    "text": "Intra-process communication\n\nCommunication between multiple threads in a process is usually accomplished using shared memory (since they naturally share it)\n\nPotential issues?\n\nSynchronization (we’ll come back to this)\n\n\nThreads within a process also share open file handles and both static and dynamically-allocated global variables.\nThread stacks and thus thread local variables are typically private."
  },
  {
    "objectID": "day6.html#why-do-we-use-threads-pros",
    "href": "day6.html#why-do-we-use-threads-pros",
    "title": "CSC 453",
    "section": "Why do we use threads? Pros",
    "text": "Why do we use threads? Pros\n\nTo support multiple activities within a single process\nResponsiveness: a process may continue to run, even if part of it is blocked (depends on thread implementation)\nResource sharing: Process are isolated so can only communicate with help from the kernel; threads share a memory space and a process’s resources (e.g. files)\nEconomy: Allocating resources for multiple processes is costly, because it involves the kernel. Threads can be handled entirely in user space and are much faster to allocate and destroy. Allows for pop-up threads, dynamically created threads to support load\nHardware support: modern CPUs support multithreading, where threads can run in parallel"
  },
  {
    "objectID": "day6.html#thread-cons",
    "href": "day6.html#thread-cons",
    "title": "CSC 453",
    "section": "Thread cons",
    "text": "Thread cons\n\nThis is hard. Burdens programmer needs to solve:\n\nDividing activities\nBalance\nData splitting\nData dependency\nTesting and debugging\n\nShare the same address space and global variable: loss of isolation. A runaway thread can wipe out other threads\nNo built-in mechanism for pre-emption, so threads should be well-behaved (unlike greedy processes)\nn threads see 1/n of a single CPU; CPU-bound processes make thread moot, or detrimental"
  },
  {
    "objectID": "day6.html#what-makes-more-sense-threads-or-process",
    "href": "day6.html#what-makes-more-sense-threads-or-process",
    "title": "CSC 453",
    "section": "What makes more sense, threads or process?",
    "text": "What makes more sense, threads or process?\n\nWeb server\nGUI apps\nMultimedia apps\nDB servers\nOS services"
  },
  {
    "objectID": "day6.html#what-makes-more-sense-threads-or-process-1",
    "href": "day6.html#what-makes-more-sense-threads-or-process-1",
    "title": "CSC 453",
    "section": "What makes more sense, threads or process?",
    "text": "What makes more sense, threads or process?\n\nWeb server - thread (through there are many hybrid implementations)\nGUI apps - thread\nMultimedia apps - thread\nDB servers - both: postgres uses processes, MySQL is multithreaded\nOS services - process"
  },
  {
    "objectID": "day6.html#side-note-fork-and-threading",
    "href": "day6.html#side-note-fork-and-threading",
    "title": "CSC 453",
    "section": "Side note: fork() and threading",
    "text": "Side note: fork() and threading\n\nWhat happens to threads when you fork()?\nShould a child inherit the threads of their parent?\nWhat happens if two threads are competing for the same resource? E.g., who gets the input from a keyboard or established network connection?\nIn reality (Linux): a child process, only the calling thread is replicated."
  },
  {
    "objectID": "day6.html#a-word-of-caution-amdahls-law",
    "href": "day6.html#a-word-of-caution-amdahls-law",
    "title": "CSC 453",
    "section": "A word of caution: Amdahl’s Law",
    "text": "A word of caution: Amdahl’s Law\n\nThreads are awesome, let’s parallelize everything, right?\nAmdahl’s Law identifies performance gains from adding additional cores to an application that has both serial and parallel components\n\n\\(S\\) is serial portion\n\\(N\\) processing cores \\[speedup \\le \\frac{1}{S + \\frac{(1-S)}{N}}\\]\n\nThat is, if application is 75% parallel / 25% serial, moving from 1 to 2 cores results in speedup of \\(1.6 \\times\\)\nAs \\(N\\) approaches infinity, speedup approaches \\(1 / S\\)\nSerial portion has a disproportionate effect on performance"
  },
  {
    "objectID": "day6.html#section-1",
    "href": "day6.html#section-1",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day6.html#questions-to-consider-2",
    "href": "day6.html#questions-to-consider-2",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat is the difference between user threads and kernel threads\nWhat are the different kernel thread models and how do they compare?"
  },
  {
    "objectID": "day6.html#first-things-first",
    "href": "day6.html#first-things-first",
    "title": "CSC 453",
    "section": "First things first",
    "text": "First things first\n\nA kernel thread is the unit of execution that the OS can run on a CPU core\nA process can only make progress if at least one of its kernel threads is scheduled on a core\nCPUs run kernel threads, not processes"
  },
  {
    "objectID": "day6.html#posix-threads-pthreads",
    "href": "day6.html#posix-threads-pthreads",
    "title": "CSC 453",
    "section": "POSIX threads (pthreads)",
    "text": "POSIX threads (pthreads)\n\n60 calls, but the important ones are: pthread*\n\ncreate, exit, yield, join (Look familiar?)\n\nPthreads were historically (pre 2003) a user space library: all functionality happens in user space, and the kernel knew nothing about them\n\ncan run on kernels or CPUs with no thread support\norders of magnitude faster than a kernel trap (everything is a local function call)\ncustom schedulers"
  },
  {
    "objectID": "day6.html#pure-user-space-threads",
    "href": "day6.html#pure-user-space-threads",
    "title": "CSC 453",
    "section": "Pure user space threads",
    "text": "Pure user space threads\n\nTotally unknown to the OS\nUser-level threads must “borrow” a kernel thread to run\nWhat happens when a thread makes an I/O-bound system call?\n\nAll threads will block, because the process is blocked\nSometimes a thread blocking a process is unavoidable: page fault: more later\n\nWhere we typically want threads is where there is a lot of I/O blocking or system calls: the web server example"
  },
  {
    "objectID": "day6.html#kernel-threads",
    "href": "day6.html#kernel-threads",
    "title": "CSC 453",
    "section": "Kernel threads",
    "text": "Kernel threads\n\nThread libraries can also have kernel support (this is the common case today)\nThere is no user space runtime environment or thread table: all is managed within the kernel\nSupport for multiple cores\nAll thread interfaces are system calls handled by the kernel"
  },
  {
    "objectID": "day6.html#kernel-threads-contd",
    "href": "day6.html#kernel-threads-contd",
    "title": "CSC 453",
    "section": "Kernel threads (cont’d)",
    "text": "Kernel threads (cont’d)\n\nInstead of blocking on a system call, the kernel now has the intelligence to schedule another thread\nCreating and destroying threads comes at a higher cost (the kernel trap we were trying to avoid above)\n\nWhat should we do?\n\nThread pools: a collection of pre-allocated threads, assigned as needed"
  },
  {
    "objectID": "day6.html#thread-design-models-many-to-one-m1",
    "href": "day6.html#thread-design-models-many-to-one-m1",
    "title": "CSC 453",
    "section": "Thread design models: many-to-one (M:1)",
    "text": "Thread design models: many-to-one (M:1)\n\nKernel threads must support user level threads. There are multiple ways to pull this off\nMany-to-one\n\nA single kernel level thread supports many user level threads\nSwitching between threads is fast (done in user level)\nNo parallelism (kernel thread is supporting only a single user thread at a time)\nIf any user thread makes an I/O blocking call, all user threads in that process are blocked\n\nWhere does this make sense?\n\nIf you need complete control over scheduling\nIf you have no kernel or minimal RTOS kernel"
  },
  {
    "objectID": "day6.html#thread-models-many-to-many-mn",
    "href": "day6.html#thread-models-many-to-many-mn",
    "title": "CSC 453",
    "section": "Thread models: many-to-many (M:N)",
    "text": "Thread models: many-to-many (M:N)\n\nSet of user threads supported by &lt;= number of kernel\nUser thread isn’t bound to a particular kernel thread\n\nIf kernel thread has to block and was supporting 3 user threads, the others can migrate\n\nAllows parallelism\nProgrammer has more control (scheduling in user space)\nSounds like the best of all worlds, right?\n\nCan be hard to implement without decent language support\nNow you have two schedulers to deal with (user space and kernel)\ngoroutines are a notable example of M:M in reality (though they are achieved through the Go runtime and not the kernel itself)"
  },
  {
    "objectID": "day6.html#thread-models-one-to-one-11",
    "href": "day6.html#thread-models-one-to-one-11",
    "title": "CSC 453",
    "section": "Thread models: one-to-one (1:1)",
    "text": "Thread models: one-to-one (1:1)\n\nNumber of kernel-level threads is equal to the number of user-level threads (dominant model today)\nCan operate in parallel\nIf one blocks, others can continue\nDownsides?\n\nLose the advantage of fast switching between user threads\nAny new user thread requires kernel thread creation: expensive\nProgrammer loses scheduling control: kernel makes decisions\n\nVery popular model: Linux & Windows\n\nEasy to implement\nMost machines have multiple cores so we can support a lot of kernel threads"
  },
  {
    "objectID": "day6.html#which-models-make-sense",
    "href": "day6.html#which-models-make-sense",
    "title": "CSC 453",
    "section": "Which models make sense?",
    "text": "Which models make sense?\n\nDistributed scientific computing\n\n1:1 - In scientific computing, you want predictable, full-speed access to hardware, not an extra runtime scheduler in the way\n\nEmbedded system\n\nM:1 - In embedded systems, simplicity and determinism matter more than scalability. You may only have a single core\n\nNGINX web servers\n\nM:N - NGINX multiplexes connections (user-level work) onto a small number of kernel threads"
  },
  {
    "objectID": "day6.html#section-2",
    "href": "day6.html#section-2",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day3.html#admin",
    "href": "day3.html#admin",
    "title": "CSC 453",
    "section": "Admin",
    "text": "Admin\n\nLab 1 due tonight\n\nWindows users: there are some settings to make the VM work if you missed lab\n\nProgramming assignment 1 due next Monday"
  },
  {
    "objectID": "day3.html#questions-to-consider",
    "href": "day3.html#questions-to-consider",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nHow do we ensure that a user process doesn’t harm others?\nHow do system calls work? How do they related to wrapper libraries like glibc?"
  },
  {
    "objectID": "day3.html#dual-mode-operation",
    "href": "day3.html#dual-mode-operation",
    "title": "CSC 453",
    "section": "Dual-mode operation",
    "text": "Dual-mode operation\n\nDual-mode operation allows OS to protect itself and components\n\nUser mode and kernel mode\n\nMode bit provided by hardware\n\nProvides ability to distinguish when system is running user code or kernel code.\nWhen a user is running → mode bit is “user”\nWhen kernel code is executing → mode bit is “kernel”\n\nSystem call changes mode to kernel, return from call resets it to user\nSome instructions are only executable in kernel mode"
  },
  {
    "objectID": "day3.html#system-calls",
    "href": "day3.html#system-calls",
    "title": "CSC 453",
    "section": "System calls",
    "text": "System calls\n\nThe OS offers a number of services. How do we (applications) interface with them?\n\nWe don’t want to deal with the details, just the abstraction\nThe OS has ultimate control over these operations\n\nSystem calls are the “language” of communication with the OS\nStandards\n\nWin32 (MS)\nPOSIX (nearly all Unix-based systems)\nJava API for the JVM"
  },
  {
    "objectID": "day3.html#system-calls-contd",
    "href": "day3.html#system-calls-contd",
    "title": "CSC 453",
    "section": "System calls (cont’d)",
    "text": "System calls (cont’d)\n\nLike a function call, we push arguments onto the stack, then we call into the library that provides the system call\nEach system call has a special number, placed into a register\nExecutes a TRAP instruction (switch to kernel mode)\nA logical separation of memory space\nKernel’s system call handler is invoked, once done (but may block) may be returned to the process"
  },
  {
    "objectID": "day3.html#system-calls-contd-1",
    "href": "day3.html#system-calls-contd-1",
    "title": "CSC 453",
    "section": "System calls (cont’d)",
    "text": "System calls (cont’d)\n\nTable defined in the kernel: https://github.com/torvalds/linux/blob/master/arch/x86/entry/ syscalls/syscall_32.tbl\n\nNote that system call tables can differ between architectures\n\nYou can run using the table values themselves using the syscall() wrapper\n\nQ: why does syscall() exist?\nIf you’re interested… There are debates https://lwn.net/Articles/771441/"
  },
  {
    "objectID": "day3.html#section",
    "href": "day3.html#section",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day3.html#questions-to-consider-1",
    "href": "day3.html#questions-to-consider-1",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat do processes contain?\nHow does the OS run multiple processes at the same time?\nHow are processes laid out in memory?\nHow does the OS store information about each process?"
  },
  {
    "objectID": "day3.html#processes",
    "href": "day3.html#processes",
    "title": "CSC 453",
    "section": "Processes",
    "text": "Processes\n\nMost fundamental OS abstraction\n\nProcesses organize information about other abstractions and represent a single thing the computer is “doing”\n\nWhen you run an executable program (passive), the OS creates a process == a running program (active)\nOne program can be multiple processes"
  },
  {
    "objectID": "day3.html#process-organization",
    "href": "day3.html#process-organization",
    "title": "CSC 453",
    "section": "Process organization",
    "text": "Process organization\n\n\n\nUnlike threads, address spaces and files, processes are not tied to a hardware component. Instead, they contain other abstractions\nProcesses contain:\n\none or more threads,\nan address space, and\nzero or more open file handles representing files"
  },
  {
    "objectID": "day3.html#multiprogramming",
    "href": "day3.html#multiprogramming",
    "title": "CSC 453",
    "section": "Multiprogramming",
    "text": "Multiprogramming\n\nProcesses are the core abstraction that allows for multiprogramming: the illusion of concurrency\nOS timeshares CPU across multiple processes: virtualizes CPU\nOS has a CPU scheduler that picks one of the many active processes to execute on a CPU\nPolicy:\n\nwhich process to run\n\nMechanism:\n\nhow to context switch between processes"
  },
  {
    "objectID": "day3.html#processs-view-of-the-world",
    "href": "day3.html#processs-view-of-the-world",
    "title": "CSC 453",
    "section": "Process’s view of the world",
    "text": "Process’s view of the world\n\n\n\nOwn memory with consistent addressing (divorced from physical addressing)\nIt has exclusivity over the CPU: It doesn’t have to worry about scheduling\nConversely, it doesn’t know when it will be scheduled, so real time events require special handling\nHas some identity: pid, gid, uid\nHas a set of services available to it via the OS\n\nData (via file system)\nCommunication (sockets, IPC)\nMore resources (e.g., memory)"
  },
  {
    "objectID": "day3.html#process-memory-layout",
    "href": "day3.html#process-memory-layout",
    "title": "CSC 453",
    "section": "Process memory layout",
    "text": "Process memory layout\n\n\n\nText segment: machine instructions; shareable between identical processes; read-only\nData segment: for initialized data; e.g., int count = 99;\nBSS (block started by symbol) segment: uninitialized data; e.g., int sum[10];\nHeap: dynamic memory allocation\nStack: initial arguments and environment; stack frames"
  },
  {
    "objectID": "day3.html#oss-view-of-the-process-world",
    "href": "day3.html#oss-view-of-the-process-world",
    "title": "CSC 453",
    "section": "OS’s view of the (process) world",
    "text": "OS’s view of the (process) world\n\n\n\nData for each process is held in a data structure known as a Process Control Block\nPartitioned memory:\n\ndedicated & shared address space\n\nperhaps non-contiguous\n\nProcess table holds PCBs"
  },
  {
    "objectID": "day3.html#section-1",
    "href": "day3.html#section-1",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day3.html#questions-to-consider-2",
    "href": "day3.html#questions-to-consider-2",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are the different process states and what causes transitions?\nWhat is a context switch?\nWhat are the two general categories of processes and how do they differ?"
  },
  {
    "objectID": "day3.html#process-states",
    "href": "day3.html#process-states",
    "title": "CSC 453",
    "section": "Process states",
    "text": "Process states\n\nAs a process executes, it changes state\n\nNew: The process is being created\nRunning: Instructions are being executed\nWaiting: The process is waiting for some event (typically I/O or signal handling) to occur\nReady: The process is waiting to be assigned to a processor\nTerminated: The process has finished execution"
  },
  {
    "objectID": "day3.html#process-state-transitions",
    "href": "day3.html#process-state-transitions",
    "title": "CSC 453",
    "section": "Process state transitions",
    "text": "Process state transitions"
  },
  {
    "objectID": "day3.html#process-state-transitions-contd",
    "href": "day3.html#process-state-transitions-contd",
    "title": "CSC 453",
    "section": "Process state transitions (cont’d)",
    "text": "Process state transitions (cont’d)\n\n\n\nRunning process can move from running to terminated (exit or killed), moved to ready (time slice up), or blocked (signaled to wait, I/O)\nWhich state transitions could happen with these expensive actions?\n\nCompute a new RSA key?\nFind the largest value in a 1TB of data?\n\n\n\n\n\n\n\n\n\nRunning to Waiting: This transition occurs when a process cannot continue executing until a specific event occurs. Here are some examples:\n\nI/O Operations:\n\nExample: A process needs to read data from a disk. It issues an I/O request and then moves to the Waiting state until the data is read and available.\nReal-World Scenario: A web server process waiting for data to be read from a database.\n\nResource Availability:\n\nExample: A process requires a resource (like a printer) that is currently in use by another process. It moves to the Waiting state until the resource becomes available.\nReal-World Scenario: A document editing application waiting for access to a shared printer.\n\nInter-Process Communication (IPC):\n\nExample: A process is waiting for a message from another process. It moves to the Waiting state until the message is received.\nReal-World Scenario: A chat application waiting for a message from a server.\n\nSynchronization Primitives:\n\nExample: A process is waiting for a lock or semaphore to be released by another process. It moves to the Waiting state until the lock is available.\nReal-World Scenario: A banking application waiting for a transaction lock to be released.\n\n\nRunning to Ready: This transition occurs when a process is preempted by the scheduler, but it is still ready to run as soon as it gets CPU time again. Here are some examples:\n\nTime Slice Expiration:\n\nExample: A process has used up its allocated time slice. The scheduler preempts it and moves it to the Ready state, allowing another process to run.\nReal-World Scenario: A video streaming application being preempted to allow a background update process to run.\n\nHigher Priority Process:\n\nExample: A higher priority process becomes ready to run. The scheduler preempts the current process and moves it to the Ready state.\nReal-World Scenario: An emergency alert system preempting a running media player application.\n\nVoluntary Yield:\n\nExample: A process voluntarily yields the CPU, indicating it can be preempted. The scheduler moves it to the Ready state.\nReal-World Scenario: A background data synchronization process yielding the CPU to allow a user-initiated task to run."
  },
  {
    "objectID": "day3.html#process-scheduling",
    "href": "day3.html#process-scheduling",
    "title": "CSC 453",
    "section": "Process scheduling",
    "text": "Process scheduling\n\nOS process scheduler selects among available processes for next execution on CPU core\nGoal?\n\nMaximize CPU use, quickly switch processes onto CPU core\n\nMaintains scheduling queues of processes\n\nReady queue: set of all processes residing in main memory, ready and waiting to execute\nWait queues: set of processes waiting for an event (i.e., I/O)\n\nProcesses migrate among the various queues over their lifetime"
  },
  {
    "objectID": "day3.html#context-switching",
    "href": "day3.html#context-switching",
    "title": "CSC 453",
    "section": "Context switching",
    "text": "Context switching\n\nWhen CPU switches to another process, the system must save the state of the old process and load the saved state for the new process via a context switch\nContext of a process represented in the PCB\nContext-switch time is pure overhead; the system does no useful work while switching\n\nThe more complex the OS and the PCB → the longer the context switch\n\nTime dependent on hardware support\n\nSome hardware provides multiple sets of registers per CPU → multiple contexts loaded at once"
  },
  {
    "objectID": "day3.html#context-switching-overhead",
    "href": "day3.html#context-switching-overhead",
    "title": "CSC 453",
    "section": "Context switching overhead",
    "text": "Context switching overhead\n\nOn the order of milliseconds\nIf not done intelligently, you can spend more time context-switching than actual processing\nQuestion: Why shouldn’t processes control context switching?\n\n\n\nThey could refuse to give up CPU (processes are greedy)\nThey’re intentionally isolated, and don’t have enough information about other processes\nIt would cause too much complication (every process would have to implement its own context switch code)"
  },
  {
    "objectID": "day3.html#scheduling-basics",
    "href": "day3.html#scheduling-basics",
    "title": "CSC 453",
    "section": "Scheduling basics",
    "text": "Scheduling basics\n\nScheduler usually makes the transition decisions; hides the details from the process/user\nProcesses often characterized as one of two types by what state they spend most of their time in\n\nI/O bound: work is dependent on I/O; e.g., browser, db, media streaming\nCPU bound: work is dependent on CPU; e.g., scientific apps, cryptography\nWhy does this matter?\n\nUnderstanding which your process is allows for optimization\n\nCPU-bound? Faster CPU, parallelize.\nI/O? Faster I/O devices, use async\n\n\n\nScheduler must balance CPU- & I/O-bound processes\n\nReminder: the goal is to maximize CPU utilization"
  },
  {
    "objectID": "day3.html#aside-multiprocessing-in-mobile",
    "href": "day3.html#aside-multiprocessing-in-mobile",
    "title": "CSC 453",
    "section": "Aside: multiprocessing in mobile",
    "text": "Aside: multiprocessing in mobile\n\nSome mobile systems (e.g., early versions of iOS) allow only one process to run, others suspended\nDue to screen real estate, user interface limits iOS provides:\n\nSingle foreground process controlled via user interface\nMultiple background processes in memory, running, but not on the display, and with limits\n\nLimits include single, short task, receiving notification of events, specific long-running tasks like audio playback\n\n\nAndroid runs foreground and background, with fewer limits\n\nBackground process uses a service to perform tasks\nService can keep running even if background process is suspended\nService has no user interface, small memory use"
  },
  {
    "objectID": "day3.html#section-2",
    "href": "day3.html#section-2",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day5.html#admin",
    "href": "day5.html#admin",
    "title": "CSC 453",
    "section": "Admin",
    "text": "Admin\n\nQuiz 3 due Friday\nLab 3 due Monday\nProgramming assignment 2 due Feb 2\nMidterm question"
  },
  {
    "objectID": "day5.html#questions-to-consider",
    "href": "day5.html#questions-to-consider",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nHow do pipes differ from FIFOs and when would you use each?\nWhat considerations matter when choosing between pipes, FIFOs, and memory-mapped files?"
  },
  {
    "objectID": "day5.html#pipes-anonymous-pipes",
    "href": "day5.html#pipes-anonymous-pipes",
    "title": "CSC 453",
    "section": "Pipes (anonymous pipes)",
    "text": "Pipes (anonymous pipes)\n\nPipes are unidirectional; one end must be designated as the reading end and the other as the writing end\nPipes are order preserving; all data read from the receiving end of the pipe will match the order in which it was written into the pipe\nPipes have a limited capacity and they use blocking I/O; if a pipe is full, any additional writes to the pipe will block the process until some of the data has been read\nPipes send data as unstructured byte streams. There are no pre-defined characteristics to the data exchanged, such as a predictable message length"
  },
  {
    "objectID": "day5.html#pipes-contd",
    "href": "day5.html#pipes-contd",
    "title": "CSC 453",
    "section": "Pipes (cont’d)",
    "text": "Pipes (cont’d)\n\nPipes create a producer-consumer buffer between two processes\nCannot be accessed outside of the creating process (child inherits the pipe since it’s a fd)\nThe operating system manages a queue for each pipe to accommodate different input and output rates\nFacilitates the canonical chaining together of small UNIX utilities to do more sophisticated processing"
  },
  {
    "objectID": "day5.html#pipe-bugs",
    "href": "day5.html#pipe-bugs",
    "title": "CSC 453",
    "section": "Pipe bugs",
    "text": "Pipe bugs\nint pipefd[2];\npipe (pipefd);\n\nif (fork () == 0)\n  exit (0);\n\nchar buffer[10];\nread (pipefd[0], buffer, sizeof (buffer));\n\nWhat will happen here?\n\nParent will block until an EOF is written into the pipe. Since the child is the only other process that could write to the pipe and the child exits without writing anything, and the parent has not closed its write end, the parent will block indefinitely."
  },
  {
    "objectID": "day5.html#named-pipes-also-known-as-fifos",
    "href": "day5.html#named-pipes-also-known-as-fifos",
    "title": "CSC 453",
    "section": "Named pipes (also known as FIFOs)",
    "text": "Named pipes (also known as FIFOs)\n\nOf course there is another type that don’t share the same characteristics\nNamed pipes are bidirectional, don’t have the parent-child relationship\n\nCreates a persistent file-like name\nRequire a reader and writer"
  },
  {
    "objectID": "day5.html#named-pipes-contd",
    "href": "day5.html#named-pipes-contd",
    "title": "CSC 453",
    "section": "Named pipes (cont’d)",
    "text": "Named pipes (cont’d)\n\nReally a data stream (ordering, buffering, reliability, authentication all implied)\nThey are not regular files, though they look like them\n\nOnce data has been read from a FIFO, the data is discarded and cannot be read again\nCannot broadcast: only one read()\nNot wise to use bidirectionally, why?"
  },
  {
    "objectID": "day5.html#shared-memory-with-mmap",
    "href": "day5.html#shared-memory-with-mmap",
    "title": "CSC 453",
    "section": "Shared memory with mmap",
    "text": "Shared memory with mmap\n\n\n\nMemory-mapped files allow for multiple processes to share read-only access to a common file. Example: libc.so mapped into all running C programs\nMemory-mapped files bypass the kernel’s buffer cache (as done with a normal read()), and the data is copied directly into the user-mode portion of memory\n\n\n\n\n\n\n\nProvide extremely fast IPC data exchange. i.e., when one process writes to the region, that data is immediately accessible by the other process without having to invoke a system call\nUnlike pipes, memory-mapped files create persistent IPC. Once the data is written to the shared region, it can be repeatedly accessed by other processes"
  },
  {
    "objectID": "day5.html#memory-mapped-files-posix-and-system-v",
    "href": "day5.html#memory-mapped-files-posix-and-system-v",
    "title": "CSC 453",
    "section": "Memory mapped files: POSIX and System V",
    "text": "Memory mapped files: POSIX and System V\n\nThese two ultimately mmap a file, but do some trickery beforehand and use special files\nPOSIX == named, RAM-backed file descriptors\n\nname → fd → mmap\nObject survives until the last reference is gone\nusually backed by tmpfs (/dev/shm)\n\nSystem V shared memory\n\nPredates POSIX - designed before file descriptors were a universal abstraction\nKernel-persistent objects, you must clean up manually\n\nProcess exits ≠ memory freed"
  },
  {
    "objectID": "day5.html#posix-vs.-system-v",
    "href": "day5.html#posix-vs.-system-v",
    "title": "CSC 453",
    "section": "POSIX vs. System V",
    "text": "POSIX vs. System V\n\nPOSIX is great, right!?\n\nPOSIX (e.g., shm_open()) IPC may not work in your favor in macOS\nSome questionable engineer (obviously not a CP grad) decided to provide the header files for certain things, but they are empty stubs\n\nIt will compile, but could be bad\n\n\nSystem V is “fine”\n\nshmget(): allocate\nshmat(): attach\nshmdt(): detach\nshmctl(): control (destroy with IPC_RMID)"
  },
  {
    "objectID": "day5.html#message-passing-vs.-shared-memory",
    "href": "day5.html#message-passing-vs.-shared-memory",
    "title": "CSC 453",
    "section": "Message passing vs. shared memory",
    "text": "Message passing vs. shared memory\n\n\n\nWhich do you choose?\n\nIf you have few messages?\nIf you have millions?\nIf you need to communicate across systems?\nIf you need in-order delivery but don’t want to code it yourself?\n\nConsiderations:\n\nCost to establish\nCost per message\n\n\n\n\n\n “Gemini, make an image in the style of a video game pitting pipes versus shared memory”"
  },
  {
    "objectID": "day5.html#section",
    "href": "day5.html#section",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day5.html#questions-to-consider-1",
    "href": "day5.html#questions-to-consider-1",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are the differences / tradeoffs between preemptive and non-preemptive scheduling algorithms?\nWhat are the metrics we can attempt to optimize scheduling for a given scenario?"
  },
  {
    "objectID": "day5.html#preemptive-vs.-non-preemptive-scheduling",
    "href": "day5.html#preemptive-vs.-non-preemptive-scheduling",
    "title": "CSC 453",
    "section": "Preemptive vs. non-preemptive scheduling",
    "text": "Preemptive vs. non-preemptive scheduling\n\nNon-Preemptive: The scheduler only makes scheduling decisions when a process voluntarily gives up the CPU (e.g., by blocking on I/O, completing, or explicitly yielding).\nPreemptive: The scheduler can interrupt a running process and move it to the ready queue at any time (based on time slices, priority changes, etc.), even while it’s actively executing.\nNon-preemptive is easier to implement, but\n\n… doesn’t support multiprogramming very well\n\nPreemptive can be complicated: correctness, utilization, flexibility\n\nSay a process P is preempted while in the middle of inserting an element in a data structure that another process (which is chosen by the scheduler) is going to process"
  },
  {
    "objectID": "day5.html#scheduling-metrics",
    "href": "day5.html#scheduling-metrics",
    "title": "CSC 453",
    "section": "Scheduling metrics",
    "text": "Scheduling metrics\n\nCPU utilization: keep the CPU as busy as possible\nThroughput: # of processes that complete their execution per time unit\nTurnaround time: amount of time to execute a particular process\nWaiting time: amount of time a process has been waiting in the ready queue\nResponse time: amount of time it takes from when a request was submitted until the first response is produced."
  },
  {
    "objectID": "day5.html#scheduling-scenarios",
    "href": "day5.html#scheduling-scenarios",
    "title": "CSC 453",
    "section": "Scheduling scenarios",
    "text": "Scheduling scenarios\nWhich metric would you optimize for these situations?\n\n\n\nYou run a data center that charges based on jobs completed\n\nMax throughput: maximize the number of jobs processed per unit time\n\nYou run GeForce Now\n\nMin response time: user experience is the main concern\n\nYou run Github Actions or Travis CI\n\nMin turnaround time: developers need feedback quickly\n\nYou run a supercomputing center\n\nMaximize CPU usage: this stuff is expensive, use it efficiently\n\n\n\n\n\n\nCPU utilization\nThroughput\nTurnaround time\nWaiting time\nResponse time"
  },
  {
    "objectID": "day5.html#what-does-the-os-scheduler-know-about-a-process",
    "href": "day5.html#what-does-the-os-scheduler-know-about-a-process",
    "title": "CSC 453",
    "section": "What does the OS scheduler know about a process?",
    "text": "What does the OS scheduler know about a process?\n\nProcess characteristics that we’d like to know in order to achieve our goals\n\nProcessing Time (Burst Time)\nI/O Requirements\nPriority\nAge\nDependencies\n\nThe scheduler knows very little of the above\nIn many cases we are hoping to predict the future. Easy, right?"
  },
  {
    "objectID": "day5.html#section-1",
    "href": "day5.html#section-1",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day5.html#questions-to-consider-2",
    "href": "day5.html#questions-to-consider-2",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nHow do FCFS, SJF, and SRTF compare in terms of tradeoffs between waiting time and response time?\nWhy does SJF require predicting future CPU bursts, and how can we estimate them?\nWhat is starvation in the context of scheduling, and how can we prevent it?"
  },
  {
    "objectID": "day5.html#batch-scheduling-fcfs",
    "href": "day5.html#batch-scheduling-fcfs",
    "title": "CSC 453",
    "section": "Batch scheduling: FCFS",
    "text": "Batch scheduling: FCFS\n\n\n\nFirst-Come, First-Served (FCFS)\nNon-preemptive\nPros:\n\nEasy to understand, easy to implement\n\nCons?\n\nWaiting time & turnaround time have high variance\nExample: 1 long CPU-bound process and many I/O bound processes\n\nConvoy effect"
  },
  {
    "objectID": "day5.html#fcfs-example",
    "href": "day5.html#fcfs-example",
    "title": "CSC 453",
    "section": "FCFS example",
    "text": "FCFS example\n\n\n\nCalculate the average waiting time:\nOrder: P1, P2, P3 \nWaiting time P1 = 0; P2 = 24; P3 = 27\nAverage waiting time: \\((0 + 24 + 27)/3 = 17\\)\nOrder P2, P3, P1 \nWaiting time P1 = 6; P2 = 0; P3 = 3\nAverage waiting time: \\((6 + 0 + 3)/3 = 3\\)\n\n\n\n\nProcess Burst Time  \n  P1         24\n  P2          3\n  P3          3"
  },
  {
    "objectID": "day5.html#okay-fcfs-can-be-bad",
    "href": "day5.html#okay-fcfs-can-be-bad",
    "title": "CSC 453",
    "section": "Okay, FCFS can be bad",
    "text": "Okay, FCFS can be bad\nCan we think of a better algorithm to deal with the reality that jobs run for different amounts of time?"
  },
  {
    "objectID": "day5.html#batch-scheduling-shortest-job-first-sjf",
    "href": "day5.html#batch-scheduling-shortest-job-first-sjf",
    "title": "CSC 453",
    "section": "Batch scheduling: shortest job first (SJF)",
    "text": "Batch scheduling: shortest job first (SJF)\n\n\n\nOrdered by length of the process’s next CPU burst\nNon-preemptive\nPros?\n\nOptimal average waiting time\n\nCons?\n\nRequires knowing the future (potentially okay in some situations; examples?)\n\nScientific jobs, batch jobs\n\n\nQuestion: how do we know the length of the upcoming CPU burst?\n\nAsk the process? Estimate? How?"
  },
  {
    "objectID": "day5.html#sjf-how-can-we-predict-the-future",
    "href": "day5.html#sjf-how-can-we-predict-the-future",
    "title": "CSC 453",
    "section": "SJF: how can we predict the future?",
    "text": "SJF: how can we predict the future?\n\nWe can’t trust a process to tell the truth\nWe can only estimate, how?\n\nThe past (with the hope that the process will act like it has before)\n\nExponential weighted moving average: \\[\nEWMA_t = \\alpha \\times x_t + (1 - \\alpha) \\times EWMA_{t-1}\n\\]\n\\(\\alpha = 0\\)? New measurement does not get taken into account\n\\(\\alpha = 1\\)? History does not get taken into account\n\\(\\alpha\\) is often set to .5"
  },
  {
    "objectID": "day5.html#sjf-example",
    "href": "day5.html#sjf-example",
    "title": "CSC 453",
    "section": "SJF example",
    "text": "SJF example\n\n\n\nGantt: \nAverage waiting time: \\((3 + 16 + 9 + 0)/4 = 7\\)\nVersus FCFS: \\((0 + 6 + 14 + 21)/4 = 10.25\\)\n\n\n\n\n    Process Burst Time  \n      P1          6\n      P2          8\n      P3          7\n      P4          3"
  },
  {
    "objectID": "day5.html#do-you-see-any-problems-with-sjf",
    "href": "day5.html#do-you-see-any-problems-with-sjf",
    "title": "CSC 453",
    "section": "Do you see any problems with SJF?",
    "text": "Do you see any problems with SJF?\n\nWhat do we do if a short process arrives after SJF has started a long running process? \nWhat should we do?\n\nPreempt"
  },
  {
    "objectID": "day5.html#shortest-remaining-time-first-srtf",
    "href": "day5.html#shortest-remaining-time-first-srtf",
    "title": "CSC 453",
    "section": "Shortest remaining time first (SRTF)",
    "text": "Shortest remaining time first (SRTF)\n\n\n\nPreemptive version of SJF\nWhenever a new process arrives in the ready queue, the decision on which process to schedule next is redone using the SJF algorithm\nQuestion: is SRTF more optimal than SJF for any metrics?\n\nYes, average waiting time is minimized"
  },
  {
    "objectID": "day5.html#srtf-example",
    "href": "day5.html#srtf-example",
    "title": "CSC 453",
    "section": "SRTF example",
    "text": "SRTF example\n    Process      Arrival Time   Burst Time\n     P1               0             8\n     P2               1             4\n     P3               2             9\n     P4               3             5\n\nAverage waiting time = \\([(0+9)+(0)+(15)+(2)]/4 = 26/4 = 6.5\\)"
  },
  {
    "objectID": "day5.html#do-you-see-any-problems-with-srtf",
    "href": "day5.html#do-you-see-any-problems-with-srtf",
    "title": "CSC 453",
    "section": "Do you see any problems with SRTF?",
    "text": "Do you see any problems with SRTF?\n\nComputers aren’t static. What if short jobs continuously arrive?\n\nLarge jobs can starve\nUrban legend about IBM 7074 at MIT: when shut down in 1973, low-priority processes were found which had been submitted in 1967 and had not yet been run…"
  },
  {
    "objectID": "day5.html#priority-scheduling",
    "href": "day5.html#priority-scheduling",
    "title": "CSC 453",
    "section": "Priority scheduling",
    "text": "Priority scheduling\n\n“Shortest” algos are examples of priority scheduling\nI.e., scheduling is based on some metric of priority\nDownsides\n\nIf not implemented carefully, indefinite blocking / starvation\nHow do we prevent starvation?\n\nAging: gradually increase the priority of processes that wait in the system for a long time"
  },
  {
    "objectID": "day5.html#section-2",
    "href": "day5.html#section-2",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day5.html#questions-to-consider-3",
    "href": "day5.html#questions-to-consider-3",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are the key differences between batch and interactive scheduling algorithms?\nHow does Round Robin improve response time compared to SJF, and what tradeoff does it introduce?\nWhy might a system benefit from using multiple scheduling queues rather than a single algorithm?"
  },
  {
    "objectID": "day5.html#batch-vs.-interactive-scheduling",
    "href": "day5.html#batch-vs.-interactive-scheduling",
    "title": "CSC 453",
    "section": "Batch vs. interactive scheduling",
    "text": "Batch vs. interactive scheduling\n\n\nBatch Scheduling\n\nOptimizes for: throughput, turnaround time, waiting time\nContext: data centers, HPC, background jobs\nUsers don’t interact with running jobs\n\n\n\n\nInteractive Scheduling\n\nOptimizes for: response time, fairness\nContext: desktop/laptop systems, servers\nUsers expect quick feedback to their actions"
  },
  {
    "objectID": "day5.html#what-about-response-time",
    "href": "day5.html#what-about-response-time",
    "title": "CSC 453",
    "section": "What about response time?",
    "text": "What about response time?\n\n\n\nSRTF is good if we know job lengths and we’re only worried about turnaround / waiting time\nSJF: poor response\nRR: better response"
  },
  {
    "objectID": "day5.html#interactive-algorithm-round-robin-rr",
    "href": "day5.html#interactive-algorithm-round-robin-rr",
    "title": "CSC 453",
    "section": "Interactive algorithm: Round Robin (RR)",
    "text": "Interactive algorithm: Round Robin (RR)\n\nFCFS with preemption\nBased on a time quantum (time slice)\n\nTypically 10-100 milliseconds\nHow do we pick a quantum?\n\nVery short: a lot of context switching (need a large quantum, otherwise overhead is too much to be worth it)\nVery long -&gt; becomes FCFS: long wait times, long turnaround times\n\nCircular queue\n\nNew processes are added to the tail\nIf a process doesn’t complete during its quantum, context switch and added to the tail"
  },
  {
    "objectID": "day5.html#rr-example",
    "href": "day5.html#rr-example",
    "title": "CSC 453",
    "section": "RR example",
    "text": "RR example\n\n\n\nAverage waiting time:  \\[[(10-4) + 4 + 7] = 17/3 = 5.66\\]\nPros:\n\nRR is fair, but do we really want fair???\nTypically, higher average turnaround than SJF, but better response\n\nCons?\n\nLong average waiting times\n\n\n\n\n\nProcess Burst Time\n  P1        24\n  P2        3\n  P3        3   \n\n  Quantum = 4"
  },
  {
    "objectID": "day5.html#gaming-rr",
    "href": "day5.html#gaming-rr",
    "title": "CSC 453",
    "section": "Gaming RR",
    "text": "Gaming RR\n\nHow could a programmer “cheat” RR?\n\nSpin up many processes (split tasks up and fork children)\nJob A: 9 processes\nJob B: 1 process\nA gets 90% of the CPU in RR\n\nThis actually happens in many systems that aim for rudimentary fairness"
  },
  {
    "objectID": "day5.html#fcfs-sjf-srtf-rr",
    "href": "day5.html#fcfs-sjf-srtf-rr",
    "title": "CSC 453",
    "section": "FCFS, SJF, SRTF, RR",
    "text": "FCFS, SJF, SRTF, RR\n\nAll have downsides\nThose that optimize turnaround / wait, can harm response time\nThose that optimize response time, can harm turnaround, wait\nWhat should we do?"
  },
  {
    "objectID": "day5.html#interactive-algorithms-multi-level-queue-mlq",
    "href": "day5.html#interactive-algorithms-multi-level-queue-mlq",
    "title": "CSC 453",
    "section": "Interactive algorithms: Multi-Level Queue (MLQ)",
    "text": "Interactive algorithms: Multi-Level Queue (MLQ)\n\nWe have classes of process, with different needs. Why not multiple schedulers satisfying those needs?\nMultilevel queue scheduler defined by the following parameters:\n\n# of queues\nScheduling algorithms for each queue\nMethod used to determine which queue a process will enter when that process needs service\nScheduling among the queues"
  },
  {
    "objectID": "day5.html#mlq",
    "href": "day5.html#mlq",
    "title": "CSC 453",
    "section": "MLQ",
    "text": "MLQ\n\n\n\nHow do we schedule this?\n\nStrict priority: low priority could starve\nTypical: time slice among queues (e.g., real-time get 50%, system gets 30%, interactive 15%, batch 5%)\nEach queue can implement a separate scheduling policy\n\nMLQ cons:\n\nStarvation\nInflexibility"
  },
  {
    "objectID": "day5.html#interactive-algorithms-multi-level-feedback-queue-mlfq",
    "href": "day5.html#interactive-algorithms-multi-level-feedback-queue-mlfq",
    "title": "CSC 453",
    "section": "Interactive algorithms: Multi-Level Feedback Queue (MLFQ)",
    "text": "Interactive algorithms: Multi-Level Feedback Queue (MLFQ)\n\nA process can move between the various queues\nMetrics for movement:\n\nProcess requirements (we serve fast processes quickly)\nOver consumption\nChange in priority\nAge"
  },
  {
    "objectID": "day5.html#mlfq-rules",
    "href": "day5.html#mlfq-rules",
    "title": "CSC 453",
    "section": "MLFQ rules",
    "text": "MLFQ rules\n\nRule 1: If Priority(A) &gt; Priority(B), A runs (B doesn’t)\nRule 2: If Priority(A)=Priority(B), A & B run in RR\nRule 3: When a job enters the system, it is placed at the highest priority (the topmost queue)\nRule 4a: If a job uses up its allotment while running, its priority is reduced (i.e., it moves down one queue)\nRule 4b: If a job gives up the CPU (for example, by performing an I/O operation) before the allotment is up, it stays at the same priority level (i.e., its allotment is reset)\nAny problems with these???"
  },
  {
    "objectID": "day5.html#mlfq-example-one-process",
    "href": "day5.html#mlfq-example-one-process",
    "title": "CSC 453",
    "section": "MLFQ example: one process",
    "text": "MLFQ example: one process"
  },
  {
    "objectID": "day5.html#mlfq-example-two-processes",
    "href": "day5.html#mlfq-example-two-processes",
    "title": "CSC 453",
    "section": "MLFQ example: two processes",
    "text": "MLFQ example: two processes\n\n\n\nScenario 1: Perfectly fine"
  },
  {
    "objectID": "day5.html#mlfq-example-two-processes-1",
    "href": "day5.html#mlfq-example-two-processes-1",
    "title": "CSC 453",
    "section": "MLFQ example: two processes",
    "text": "MLFQ example: two processes\n\n\n\nScenario 1: Perfectly fine\nScenario 2: programmer can game the algorithm to remain at high priority"
  },
  {
    "objectID": "day5.html#mlfq-example-two-processes-2",
    "href": "day5.html#mlfq-example-two-processes-2",
    "title": "CSC 453",
    "section": "MLFQ example: two processes",
    "text": "MLFQ example: two processes\n\n\n\nScenario 1: Perfectly fine\nScenario 2: programmer can game the algorithm to remain at high priority\nScenario 3: multiple small processes keep lead to starvation"
  },
  {
    "objectID": "day5.html#mlfq-solving-gaming",
    "href": "day5.html#mlfq-solving-gaming",
    "title": "CSC 453",
    "section": "MLFQ: solving gaming",
    "text": "MLFQ: solving gaming\n\n\n\nRule 4 (new): Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue)"
  },
  {
    "objectID": "day5.html#mlfq-solving-starvation",
    "href": "day5.html#mlfq-solving-starvation",
    "title": "CSC 453",
    "section": "MLFQ: solving starvation",
    "text": "MLFQ: solving starvation\n\n\n\nRule 5: After some time period, move all the jobs in the system to the topmost queue\n\nSolves starvation\nIf a long running process evolves to be more interactive, it gets promoted"
  },
  {
    "objectID": "day5.html#what-scheduler-should-we-use",
    "href": "day5.html#what-scheduler-should-we-use",
    "title": "CSC 453",
    "section": "What scheduler should we use?",
    "text": "What scheduler should we use?\n\nTicket booking system\n\nFCFS: fairness\n\nPrint server\n\nSJF could make sense\n\nWeb server\n\nRR: fairness\n\nStreaming service\n\nSRTF: need to quickly respond to something going wrong e.g., buffering\n\nGeneral operating systems\n\nMLFQ: need to handle a mix"
  },
  {
    "objectID": "day5.html#section-3",
    "href": "day5.html#section-3",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day4.html#admin",
    "href": "day4.html#admin",
    "title": "CSC 453",
    "section": "Admin",
    "text": "Admin\n\nQuiz 2 due Friday\nLab 2 due Monday\nSLOsh due Monday\nNo class/lab Tuesday"
  },
  {
    "objectID": "day4.html#questions-to-consider",
    "href": "day4.html#questions-to-consider",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhich system calls are related to process management and lifecycles?\nHow does the process hierarchy work?\nWhat are zombies and orphans? Why do zombies exist?"
  },
  {
    "objectID": "day4.html#unix-process-apis",
    "href": "day4.html#unix-process-apis",
    "title": "CSC 453",
    "section": "UNIX process APIs",
    "text": "UNIX process APIs\n\nfork() creates a new child process\n\nAll processes are created by forking from a parent\nThe init process is ancestor of all processes\n\nRun pstree in a terminal to see\n\n\nexec() makes a process execute a given executable (effectively replaces the process)\nexit() terminates a process\nwait() causes a parent to block until child terminates\nMany variants exist of the above system calls with different arguments"
  },
  {
    "objectID": "day4.html#what-happens-during-a-fork",
    "href": "day4.html#what-happens-during-a-fork",
    "title": "CSC 453",
    "section": "What happens during a fork()?",
    "text": "What happens during a fork()?\n\nA new process is created by making a copy of parent’s memory image\nBoth parent and child have unique address spaces (isolated from each other, allowing for independent processing)\nThe new process is added to the OS process list and scheduled\nParent and child start execution just after fork (with different return values)\nParent and child execute and modify the memory data independently"
  },
  {
    "objectID": "day4.html#process-management-1",
    "href": "day4.html#process-management-1",
    "title": "CSC 453",
    "section": "Process management",
    "text": "Process management"
  },
  {
    "objectID": "day4.html#process-creation",
    "href": "day4.html#process-creation",
    "title": "CSC 453",
    "section": "Process creation",
    "text": "Process creation\n\nDifferent execution models\n\nParent & child may execute independently\nParent may wait for child\nChild may create more children (Process hierarchies)\nParent may kill children\n\nChild often invokes exec() to change its memory image to a new program\nWhy two steps (fork() then exec())?\n\nAllows the child to change file descriptors and other settings before exec()"
  },
  {
    "objectID": "day4.html#process-destruction",
    "href": "day4.html#process-destruction",
    "title": "CSC 453",
    "section": "Process destruction",
    "text": "Process destruction\n\nSome operating systems do not allow child to exist if its parent has terminated. If a process terminates, then all its children must also be terminated\n\nCascading termination: All children, grandchildren, etc., are terminated.\nThe termination is initiated by the operating system\n\nThe parent process may wait for termination of a child process by using the wait() system call. The call returns status information and the pid of the terminated process\npid = wait(&status);"
  },
  {
    "objectID": "day4.html#zombies-and-orphans",
    "href": "day4.html#zombies-and-orphans",
    "title": "CSC 453",
    "section": "Zombies and orphans",
    "text": "Zombies and orphans\n\n\n\nIf no parent waiting (did not invoke wait()), and process completes, process is a zombie\n\nZombie = dead but not yet reaped (exit status hasn’t been read)\nStill has an entry in the process table\nWe need zombies: so the kernel can preserve a child’s exit status until the parent calls wait(), even if the child exits first\n\nIf parent terminated without invoking wait(), process is an orphan\n\nOrphan = alive but parent is gone\ninit benevolently adopts orphans\n\n\n\n\n\n\n\n\nChild exits → zombie\nParent exits → zombie becomes orphan\ninit adopts it → init calls wait() → zombie disappears"
  },
  {
    "objectID": "day4.html#section",
    "href": "day4.html#section",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day4.html#questions-to-consider-1",
    "href": "day4.html#questions-to-consider-1",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are namespaces and cgroups?\nHow do they differ from virtual machines?\nHow do containers use them to isolate processes?"
  },
  {
    "objectID": "day4.html#isolation-without-full-virtualization",
    "href": "day4.html#isolation-without-full-virtualization",
    "title": "CSC 453",
    "section": "Isolation without full virtualization",
    "text": "Isolation without full virtualization\n\nVirtual machines provide complete isolation by emulating entire hardware + OS\n\nHeavier resource overhead (multiple OS instances)\nBetter isolation between workloads\n\nContainers provide lightweight isolation using OS-level mechanisms\n\nShare the same kernel\nMuch lower overhead than VMs\nLinux kernel provides the building blocks: namespaces and cgroups"
  },
  {
    "objectID": "day4.html#namespaces-logical-isolation",
    "href": "day4.html#namespaces-logical-isolation",
    "title": "CSC 453",
    "section": "Namespaces: logical isolation",
    "text": "Namespaces: logical isolation\n\nNamespaces partition global system resources so they appear as separate isolated instances\nEach process belongs to a namespace and only sees resources in that namespace\nTypes of namespaces:\n\nPID: process IDs (what processes can a process see?)\nNetwork: network interfaces, ports, routing tables\nMount: filesystem mounts (what can a process access?)\nIPC: IPC objects, message queues\nUTS: hostname and domain name\nUser: user and group IDs (who owns the process?)"
  },
  {
    "objectID": "day4.html#namespaces-example",
    "href": "day4.html#namespaces-example",
    "title": "CSC 453",
    "section": "Namespaces example",
    "text": "Namespaces example\n\nTwo processes in separate PID namespaces think they are PID 1 (init)\nEach sees only processes within their own namespace\nFrom the host OS perspective, they have different global PIDs\nEnables the illusion that each container has its own isolated process tree"
  },
  {
    "objectID": "day4.html#seeing-namespaces-in-action",
    "href": "day4.html#seeing-namespaces-in-action",
    "title": "CSC 453",
    "section": "Seeing namespaces in action",
    "text": "Seeing namespaces in action\n\nView namespaces a process belongs to:\nls -l /proc/self/ns/\nUse unshare to create a new PID namespace:\nunshare -pf --mount-proc bash      # creates new PID namespace, your shell is PID 1\nps aux               # only sees processes in this namespace\nexit                 # back to host namespace\nps aux               # will show all processes on the host\nCompare namespace inodes before and after (same inode = same namespace):\nls -i /proc/self/ns/pid\nunshare -pf --mount-proc bash -c 'ls -i /proc/self/ns/pid'  # different inode"
  },
  {
    "objectID": "day4.html#cgroups-resource-limits",
    "href": "day4.html#cgroups-resource-limits",
    "title": "CSC 453",
    "section": "Cgroups: resource limits",
    "text": "Cgroups: resource limits\n\ncgroups (control groups) limit, prioritize, and account for resource usage of process groups\nKey capabilities:\n\nCPU limits: restrict how much CPU time a group can use\nMemory limits: cap memory usage; OOM killer invoked if exceeded\nI/O limits: restrict disk I/O bandwidth\nDevice access: restrict which devices a process can access\n\nAll processes in a cgroup share the same resource limitations"
  },
  {
    "objectID": "day4.html#seeing-cgroups-in-action",
    "href": "day4.html#seeing-cgroups-in-action",
    "title": "CSC 453",
    "section": "Seeing cgroups in action",
    "text": "Seeing cgroups in action\n\nView what cgroup a process belongs to:\ncat /proc/self/cgroup\nCheck your current limits:\ncat /proc/self/limits  # shows per-process limits (some enforced by cgroups)\nIn practice, cgroups are invisible to users, kernel enforces limits automatically when a process exceeds allocated resources"
  },
  {
    "objectID": "day4.html#cgroups-vs.-namespaces",
    "href": "day4.html#cgroups-vs.-namespaces",
    "title": "CSC 453",
    "section": "Cgroups vs. namespaces",
    "text": "Cgroups vs. namespaces\n\nNamespaces: about visibility—what can a process see?\n\nLogical isolation of resources\n\ncgroups: about limits—how much can a process use?\n\nResource accounting and enforcement\n\nTogether: processes appear isolated AND are prevented from consuming excessive resources"
  },
  {
    "objectID": "day4.html#containers-as-a-building-block",
    "href": "day4.html#containers-as-a-building-block",
    "title": "CSC 453",
    "section": "Containers as a building block",
    "text": "Containers as a building block\n\ncgroups and namespaces are mechanisms, containers allow us to apply policy through orchestration\nContainers (e.g. Docker) combine namespaces + cgroups + layered filesystems\nResults in lightweight, portable process isolation\nSingle kernel, multiple isolated environments\nMuch cheaper than VMs, but with less isolation guarantees"
  },
  {
    "objectID": "day4.html#section-1",
    "href": "day4.html#section-1",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day4.html#questions-to-consider-2",
    "href": "day4.html#questions-to-consider-2",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are the two main strategies for IPC?\nHow do they differ?\nIn what situations would you choose one over the other?"
  },
  {
    "objectID": "day4.html#processes-give-us-a-protection-boundary",
    "href": "day4.html#processes-give-us-a-protection-boundary",
    "title": "CSC 453",
    "section": "Processes give us a protection boundary",
    "text": "Processes give us a protection boundary\n\nThe operating system is responsible for isolating processes from each other\nWhat you do in your own process is your own business but it shouldn’t be able to crash the machine or affect other processes, or at least processes started by other users\nThus: safe intra-process communication is your problem; safe inter-process communication is an operating system problem"
  },
  {
    "objectID": "day4.html#why-do-we-need-ipc-what-are-the-benefits",
    "href": "day4.html#why-do-we-need-ipc-what-are-the-benefits",
    "title": "CSC 453",
    "section": "Why do we need IPC? What are the benefits?",
    "text": "Why do we need IPC? What are the benefits?\n\nData Sharing: IPC allows processes to share data efficiently, which is crucial for applications requiring real-time data exchange\nModularity: It promotes modularity by enabling different parts of a system to communicate, making the system easier to manage and scale\nResource Utilization: IPC can help optimize resource utilization by allowing processes to coordinate their use of shared resources\nConcurrency (scalability): It supports concurrent execution of processes, improving the overall performance and responsiveness of applications"
  },
  {
    "objectID": "day4.html#what-are-the-disadvantages-of-ipc",
    "href": "day4.html#what-are-the-disadvantages-of-ipc",
    "title": "CSC 453",
    "section": "What are the disadvantages of IPC?",
    "text": "What are the disadvantages of IPC?\n\nComplexity: Implementing IPC can add complexity to the system, requiring careful design and management to avoid issues like deadlocks and race conditions\nOverhead: IPC mechanisms can introduce overhead, potentially impacting performance, especially if the communication is frequent or involves large amounts of data\nSecurity: Ensuring secure IPC can be challenging, as it involves protecting data from unauthorized access and ensuring the integrity of the communication\nDebugging: Debugging IPC-related issues can be difficult, as problems may arise from interactions between multiple processes, making them harder to isolate and resolve"
  },
  {
    "objectID": "day4.html#what-are-the-two-main-categories-of-ipc",
    "href": "day4.html#what-are-the-two-main-categories-of-ipc",
    "title": "CSC 453",
    "section": "What are the two main categories of IPC?",
    "text": "What are the two main categories of IPC?\n\nMessage passing\n\nHigh-level abstraction for exchanging packets of information over some interconnect\n\nShared memory\n\nRegion of memory available to different processes; writable by at least one process"
  },
  {
    "objectID": "day4.html#message-passing",
    "href": "day4.html#message-passing",
    "title": "CSC 453",
    "section": "Message passing",
    "text": "Message passing\n\nKernel establishes and oversees all communication\n\nProcess copies data to buffer, then issue system call to request transfer\nKernel copies data into its memory\nLater, process issues system call to retrieve\n\nTwo primitives: send() and recv()\nBeyond intra-computer communication, facilitates processes over a network; link implementation is unimportant"
  },
  {
    "objectID": "day4.html#pros-and-cons-of-message-passing",
    "href": "day4.html#pros-and-cons-of-message-passing",
    "title": "CSC 453",
    "section": "Pros and cons of message passing?",
    "text": "Pros and cons of message passing?\n\nPros:\n\nEasier to implement and manage, especially in distributed systems\nProvides clear boundaries between processes, enhancing security and modularity\n\nCons:\n\nCan introduce overhead due to the need for message formatting and transmission\nMay be slower compared to shared memory for large volumes of data"
  },
  {
    "objectID": "day4.html#shared-memory",
    "href": "day4.html#shared-memory",
    "title": "CSC 453",
    "section": "Shared memory",
    "text": "Shared memory\n\nKernel plays a role in establishing and attaching the address space, but does not control read/write access beyond that\nHow the memory is shared, and kept consistent, is left up to the processes"
  },
  {
    "objectID": "day4.html#pros-and-cons-of-shared-memory",
    "href": "day4.html#pros-and-cons-of-shared-memory",
    "title": "CSC 453",
    "section": "Pros and cons of shared memory?",
    "text": "Pros and cons of shared memory?\n\nPros:\n\nOffers high-speed data exchange, as processes can directly read and write to the shared memory\nEfficient for large volumes of data\n\nCons:\n\nRequires careful synchronization to avoid conflicts and ensure data consistency\nCan be more complex to implement and debug"
  },
  {
    "objectID": "day4.html#message-passing-vs.-shared-memory",
    "href": "day4.html#message-passing-vs.-shared-memory",
    "title": "CSC 453",
    "section": "Message passing vs. shared memory",
    "text": "Message passing vs. shared memory\n\n\n\nWhich do you choose?\n\nIf you have few messages?\nIf you have millions?\nIf you need to communicate across systems?\nIf you need in-order delivery but don’t want to code it yourself?\n\nConsiderations:\n\nCost to establish\nCost per message\n\n\n\n\n\n “Gemini, make an image in the style of a video game pitting pipes versus shared memory”"
  },
  {
    "objectID": "day4.html#section-2",
    "href": "day4.html#section-2",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day1.html#who-am-i",
    "href": "day1.html#who-am-i",
    "title": "CSC 453",
    "section": "Who am I?",
    "text": "Who am I?\n\nhttps://pschmitt.net\nAcademics\n\nPhD at UCSB\nResearcher (summers) at ICSI\nPostdoc then research faculty at Princeton\nSome other stops… USC, Stanford, UH\n\nPrivacy startup co-founder & CEO, measurement startup COO\nMain focuses\n\nIntegrating privacy and security into systems that we all use\nML for network management\nConnectivity / privacy in challenging environments"
  },
  {
    "objectID": "day1.html#this-class-1",
    "href": "day1.html#this-class-1",
    "title": "CSC 453",
    "section": "This class",
    "text": "This class\n\nToo much to cover in one term: survey of important topics\nCourse Canvas: https://canvas.calpoly.edu/courses/168889\nSyllabus: https://schmittpaul.github.io/CSC453W26/syllabus.pdf\nWaitlists…\nWhat OSes do you all run?"
  },
  {
    "objectID": "day1.html#who-are-you",
    "href": "day1.html#who-are-you",
    "title": "CSC 453",
    "section": "Who are you?",
    "text": "Who are you?\n\nName / Major / Year\nWhere do you identify as your hometown?"
  },
  {
    "objectID": "day1.html#how-to-succeed-in-this-class",
    "href": "day1.html#how-to-succeed-in-this-class",
    "title": "CSC 453",
    "section": "How to succeed in this class",
    "text": "How to succeed in this class\n\nTurn things in on time\nAttend class\n\n… on time. Lectures will start at 10 after the hour\n\n\nAsk questions\n\nI’m very flexible about how much we cover this semester\nI would rather teach less and have everyone understand it\nOur back-and-forth during class is the one of the few indicators I have of how much you are absorbing\n\nTalk to me if you are struggling"
  },
  {
    "objectID": "day1.html#what-are-we-doing-here",
    "href": "day1.html#what-are-we-doing-here",
    "title": "CSC 453",
    "section": "What are we doing here?",
    "text": "What are we doing here?\n\n\n\nHow many of you have participated in OS development?\nHow many of you regularly program in languages that use operating system abstractions directly?\nAnd C is a decreasingly popular language!\nSo why study operating systems? Why is this class even offered? Why is it required?"
  },
  {
    "objectID": "day1.html#why-take-this-class",
    "href": "day1.html#why-take-this-class",
    "title": "CSC 453",
    "section": "Why take this class",
    "text": "Why take this class\n\nYou are required to do so in order to graduate\nReality: this is how computers really work, and as a computer scientist or engineer you should know how computers really work\nUbiquity: operating systems are everywhere and you are likely to eventually encounter them or their limitations\nBeauty: operating systems are examples of mature solutions to difficult design and engineering problems. Studying them will improve your ability to design and implement abstractions"
  },
  {
    "objectID": "day1.html#course-progression",
    "href": "day1.html#course-progression",
    "title": "CSC 453",
    "section": "Course progression",
    "text": "Course progression\n\nIntroduction to operating system abstractions and structures\nAbstracting and multiplexing:\n\nCPU: interrupts, context, threads, processes, processor scheduling, thread synchronization, deadlocks\nMemory: memory layout, address translation, paging and segmentation, address spaces, translation caching, page fault handling, page eviction, swapping\nFile systems and storage: disk scheduling, on-disk layout, files, buffer cache, crash and recovery\nSecurity (time permitting)"
  },
  {
    "objectID": "day1.html#questions-to-consider",
    "href": "day1.html#questions-to-consider",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat is an operating system?\nWhat do they do?\nHow did OSes evolve?\nHow do the requirements differ for different OSes?"
  },
  {
    "objectID": "day1.html#what-is-an-operating-system",
    "href": "day1.html#what-is-an-operating-system",
    "title": "CSC 453",
    "section": "What is an operating system?",
    "text": "What is an operating system?\n\n\n\nOperating System:\n\nA computer program that\nmultiplexes hardware resources and\nimplements useful abstractions.\n\nThe OS is just another computer program. Has the highest privilege. The core of the OS consists of the kernel\nMultiplexing allows multiple people or programs to use the same set of hardware resources - processors, memory, disks, network connection - safely and efficiently.\nAbstractions simplify the usage of hardware resources by organizing information or implementing new capabilities."
  },
  {
    "objectID": "day1.html#os-history---how-did-we-get-here",
    "href": "day1.html#os-history---how-did-we-get-here",
    "title": "CSC 453",
    "section": "OS history - how did we get here?",
    "text": "OS history - how did we get here?\n\n\n\nStarted out as libraries to provide common functionality across programs\n\nSee any issues with this approach?\n\nLater, evolved from procedure call to system call\n\nWhat’s the difference?\n\nEvolved from running a single program to multiple processes concurrently\n\nNew issues to solve?\n\n\n\n\n\n\n\n\n\nImagine you allowed any process to read anywhere from the disk or memory. There is no privacy.\nMemory protection, concurrency / scheduling issues"
  },
  {
    "objectID": "day1.html#types-of-operating-systems",
    "href": "day1.html#types-of-operating-systems",
    "title": "CSC 453",
    "section": "Types of operating systems",
    "text": "Types of operating systems\n\nDesktop\nTime share/Mainframe\nMobile\nWeb\nReal-time\nEmbedded\nVirtual Machines"
  },
  {
    "objectID": "day1.html#tldr",
    "href": "day1.html#tldr",
    "title": "CSC 453",
    "section": "Tl;dr",
    "text": "Tl;dr\nOSes continue to evolve as environments (i.e., constraints) change"
  },
  {
    "objectID": "day1.html#section-1",
    "href": "day1.html#section-1",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  },
  {
    "objectID": "day1.html#questions-to-consider-1",
    "href": "day1.html#questions-to-consider-1",
    "title": "CSC 453",
    "section": "Questions to consider",
    "text": "Questions to consider\n\nWhat are the main abstractions OSes provide?\nWhat are the abstraction challenges?"
  },
  {
    "objectID": "day1.html#abstractions",
    "href": "day1.html#abstractions",
    "title": "CSC 453",
    "section": "Abstractions",
    "text": "Abstractions\n\nAbstractions simplify application design by:\n\nhiding undesirable properties,\nadding new capabilities, and\norganizing information\n\nAbstractions provide an interface to application programmers that separates policy—what the interface commits to accomplishing—from mechanism—how the interface is implemented."
  },
  {
    "objectID": "day1.html#what-are-the-abstractions",
    "href": "day1.html#what-are-the-abstractions",
    "title": "CSC 453",
    "section": "What are the abstractions?",
    "text": "What are the abstractions?\n\nCPUs\n\nProcesses, threads\n\nMemory\n\nAddress space\n\nDisk\n\nFiles"
  },
  {
    "objectID": "day1.html#example-os-abstraction-file-systems",
    "href": "day1.html#example-os-abstraction-file-systems",
    "title": "CSC 453",
    "section": "Example OS abstraction: file systems",
    "text": "Example OS abstraction: file systems\n\nWhat undesirable properties do file systems hide?\n\nDisks are slow!\nChunks of storage are actually distributed all over the disk\nDisk storage may fail!\n\nWhat new capabilities do files add?\n\nGrowth and shrinking\nOrganization into directories, searchability\n\nWhat information do files help organize?\n\nOwnership and permissions\nAccess time, modification time, type, etc."
  },
  {
    "objectID": "day1.html#abstraction-tradeoffs---discussion",
    "href": "day1.html#abstraction-tradeoffs---discussion",
    "title": "CSC 453",
    "section": "Abstraction tradeoffs - discussion",
    "text": "Abstraction tradeoffs - discussion\n\nIdentify undesirable properties hidden by, new capabilities added, and info organization provided with these abstractions:\n\nProcess / threads\nAddress space\n\n\n\nProcesses/Threads\n\nHiding undesirable properties:\n\nExample: The process abstraction hides the complexity of CPU scheduling and context switching. Applications don’t need to manage the low-level details of how the CPU switches between different tasks. Similarly, the thread abstraction hides the intricacies of managing multiple execution paths within a single process, allowing developers to focus on the logic of concurrent tasks.\n\nAdding new capabilities:\n\nExample: Processes provide isolation between different applications, ensuring that one misbehaving application doesn’t affect others. Threads within a process allow for parallel execution of tasks, improving performance and responsiveness. Additionally, operating systems often provide inter-process communication (IPC) mechanisms, enabling processes to coordinate and share data.\n\nOrganizing information:\n\nExample: The process abstraction organizes the execution environment by encapsulating the code, data, and resources needed for a program to run. Threads further organize execution by dividing tasks within a process into smaller, manageable units. This organization helps in structuring complex applications and improving their maintainability.\n\n\nMemory Address Space\n\nHiding undesirable properties:\n\nExample: The memory address space abstraction hides the physical memory layout from applications. Programs use virtual addresses, which the operating system maps to physical memory locations. This abstraction also hides the details of memory fragmentation and allocation, simplifying memory management for developers.\n\nAdding new capabilities:\n\nExample: Virtual memory allows applications to use more memory than physically available by swapping data to and from disk. This capability enables larger and more complex applications to run on systems with limited physical memory. Additionally, memory protection mechanisms prevent one process from accessing the memory of another, enhancing system stability and security.\n\nOrganizing information:\n\nExample: The memory address space abstraction organizes memory into segments such as code, data, heap, and stack. This organization helps in managing different types of data efficiently and ensures that memory is used in a structured manner. For instance, the stack is used for function calls and local variables, while the heap is used for dynamic memory allocation."
  },
  {
    "objectID": "day1.html#abstraction-pros-cons",
    "href": "day1.html#abstraction-pros-cons",
    "title": "CSC 453",
    "section": "Abstraction pros / cons",
    "text": "Abstraction pros / cons\n\nAdvantages of OS providing abstractions?\n\nAllow applications to reuse common facilities\nMake different devices look the same\nProvide higher-level or more useful functionality\n\nChallenges?\n\nWhat are the correct abstractions?\nHow much should be exposed?"
  },
  {
    "objectID": "day1.html#os-design-requirements---what-do-we-need",
    "href": "day1.html#os-design-requirements---what-do-we-need",
    "title": "CSC 453",
    "section": "OS design requirements - what do we need?",
    "text": "OS design requirements - what do we need?\n\nConvenience, abstraction of hardware resources for user programs\nEfficiency of usage of CPU, memory, etc.\nIsolation between multiple processes\nReliability, the OS must not fail\nOther:\n\nSecurity\nMobility"
  },
  {
    "objectID": "day1.html#section-2",
    "href": "day1.html#section-2",
    "title": "CSC 453",
    "section": "",
    "text": "What isn’t clear?\nComments? Thoughts?"
  }
]