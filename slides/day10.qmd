---
title: CSC 453
subtitle: Winter 2026 Day 10
format: 
  clean-revealjs:
    self-contained: true
    incremental: true
    margin: 0.15 
    # height: 700   # Optional: Normal height (defaults to 700)
    # width: 1000   # Optional: Normal width (defaults to 900)
---

## Admin
:::{.nonincremental}
- Program 3: we need to cover page replacement first (should be Thursday)
  - It's in C or python, your choice
:::

## Swapping
- Swapping allows an entire process to be moved from main memory to a backing store (like disk)
- Q: Why swap?
  - Can run far more processes than RAM can handle
- Q: When do we swap?
  - When we want to run more processes than can fit into physical memory
- Q: When swapping back in, where does the process go?
  - Depends on the memory binding
  - If runtime, then it can go anywhere (maximum flexibility)

## Swapping caveats
- Remember though: disk is very slow, so we must be careful and clever about who, how, and when we swap
  - The amount of time we spend swapping is directly proportional to the amount of memory we want to swap
  - Transfer time dominates (not context switching)
- The state of a process matters
  - E.g. if it's blocked on I/O, we may 1) further delay the I/O by using the disk to swap, 2) a direct I/O request may return to the wrong process
  - Programs are growing larger, so quite expensive to swap
    - e.g. 1GB program ~10 sec per swap to a spinning disk
- In reality: we don't really swap entire processes; too costly and unnecessary
  - We can swap [pages]{.alert}, we'll talk about this later

## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear?

Comments? Thoughts?
:::

# Contiguous allocation {background-color="#40666e"}

## Questions to consider
:::{.nonincremental}
- How do we keep track of free memory and decide where to place processes?
- What problems arise from fragmentation, and how can we address them?
- What are the trade-offs between different allocation strategies?
:::

## Memory allocation strategies
- How do we find free memory for processes?
- How do we keep track of free memory?
- How do we decide which free block to allocate to a process?

## Contiguous allocation
- Memory is allocated in contiguous blocks
- One approach: memory may be partitioned into *fixed-sized partitions*, each containing one process
  - This is very simple, but inflexible
  - A fixed number of partitions means we can only run n processes, and the size of the partitions may not match the needs of the processes
- Another approach: *variable-sized partitions*, where we allocate exactly as much memory as a process needs

## Variable partitioning
- Any obvious problems with variable partitioning?
  - [External fragmentation]{.alert}: over time, as processes are loaded and unloaded, we can end up with small free blocks of memory that are not usable for new processes
- To begin, we allocate memory for processes until none fit into any of the "holes"
- Once there is no hole that is big enough to fit a process, what do we do?
  - Can we rearrange memory to create enough space?
    - [Compaction]{.alert}: move processes around to create enough contiguous space for the new process
    - This is **expensive**

## Allocation strategies
- How do we decide which hole to use for a new process?
- [First fit]{.alert}: allocate the first hole that is big enough
- [Best fit]{.alert}: allocate the smallest hole that is big enough
- [Worst fit]{.alert}: allocate the largest hole
- Downsides to these?
  - First fit: can lead to fragmentation at the beginning of memory
  - Best/worst fit: require full scan
  - One solution: [Next fit]{.alert}: similar to first fit, but continues searching from the last allocated hole

## External fragmentation
- Ideally, we want all memory to be allocated, but
- In reality, we're left with many holes that are not one is big enough to allocate anything into 
  - Although, combined they may be
- All the allocation strategies above can suffer from external fragmentation
- Generally waste 1/3 of the average process size due to external frag using first-fit or best-fit
- One solution: compaction, but this is expensive

## Segmentation
- Previous approaches have been focused on allocating contiguous blocks of memory, but what if we could allow processes to be allocated in non-contiguous segments?
- [Segmentation]{.alert} divides a process's address space into logical segments (e.g., code, data, stack) that can be allocated separately
- Each segment has a base and bounds, and the OS maintains a segment table for each process that maps virtual segment numbers to physical memory locations
- Still suffers from internal fragmentation if segments are not fully utilized
- Can also lead to external fragmentation, but less so than contiguous allocation since segments can be allocated in non-contiguous memory

## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear?

Comments? Thoughts?
:::

# Paging {background-color="#40666e"}

## Questions to consider
:::{.nonincremental}
- How does paging eliminate external fragmentation while supporting multiple processes?
- How does the hardware (MMU) translate virtual addresses to physical addresses?
- What information must the OS maintain to support paging?
:::

## Paging
- Contiguous allocation schemes have many issues (particularly with finding / managing places to put processes), what should we do? 
  - Be non-contiguous!
    - Of course this introduces many new challenges, but alas
- To completely eliminate external fragmentation, we can use [paging]{.alert}
  - Non-contiguous memory allocation scheme that divides memory into fixed-size blocks called pages
  - Requires hardware support (MMU) to translate virtual addresses to physical addresses using a page table

## Paging (cont'd)
- Jargon!
  - Physical memory is broken into fixed-sized regions: **frames**; logical memory is broken into fixed-sized regions: **pages**; backing store is also addressed as fixed-sized blocks
- The paging system handles the mapping of pages to frames
- Pages may be logically contiguous, whereas the backing frames and blocks are not
- ~Typically |frames| = |pages| = |blocks|; typically between 512K-16MB
  - Most common size is 4KB these days

## Paging (cont'd)
:::: {.columns}
::: {.column width="58%"}
- Addresses are divided into two parts: [page number]{.alert} and [page offset]{.alert}
  - Page number is simply an index to a page
  - Offset is for addressing within the given page
- Recall that the process point of view is using virtual addressing
  - This means we need to bolt address translation into this. What part of the address should we translate?
  - Page number only. Offset remains the offset
:::
::: {.column width="2%"}
:::
::: {.column width="38%"}
![](images/page_offset.png)
:::
::::


## Paging (cont'd)
- Now we need a directory to convert between virtual page numbers and physical page framesâ€¦ The [page table]{.alert}
  - Can be any of a number of different data structures
  - The MMU uses the page table to perform address translation on every memory access
- Mapping is completely transparent to the user / process. OS is in control of the page table
- Page tables are kept by OS *for each process*
  - Where should it be stored?
  - Pointer held in the PCB; more to deal with on context switch

## Page tables
:::: {.columns}
::: {.column width="58%"}
- $2^m$ is the size of the address space (in bytes), and page size is $2^n$ bytes, then the higher $m-n$ bits designate page number; $n$ low-order bits are page offset
  - $2^4$ (this example) = 16 pages; page size $2^{12}$ = 4K
  - 32-bit?
    - $2^{20}$ pages (1M), 4K pages = max 4GB RAM 
  - 64-bit?
    - We'll touch on it later, it isn't (currently) $2^{52}$ 


:::
::: {.column width="2%"}
:::
::: {.column width="38%"}
![](images/pagetable.png)
:::
::::
- Using n offset bits, we can address all 4096 bytes within the page

## Examples
- A system uses a 32-bit virtual address and 8 bits are used for the offset within a page. How many pages? Size of each page? Total size?
  - $2^{24}$ pages (~16M), 256 byte pages, 4GB total
- Virtual address is 24 bits long. If 10 bits used for the page index.
  - $2^{10}$ = 1024 pages, $2^{14}$ = 16KB pages, 16MB total size
- A system has 256 pages and each page is 8 KB.
  - 8 bits for index ($2^8$ = 256), 13 bits for offset ($2^{13}$ = 8KB), 21 bit total space = $2^{21}$ = 2MB
- A system uses a 36-bit virtual address. If the page size is 4 KB
  - $2^{24}$ pages (16MB), 12-bit offset, ~64GB total

## Page tables (cont'd)
- Page Size
  - Depends greatly on system usage
  - Smaller provides granularity, but a larger page table, more overhead
  - Larger increases throughput and minimizes disk I/O
- How do pages affect fragmentation?
  - **No external frag**
  - But now we have [internal fragmentation]{.alert}; unless a process is incredibly using some multiple of page size
  - On average: $1/2$ page size per process 

## Page table entries
- Page table entry:  what goes into one?
  - Page frame number (obvi)
  - [Present/absent]{.alert} bit: is the page frame number valid?
  - [Protection bits]{.alert}: What kinds of access are permitted 
  - [Modified/Dirty]{.alert} bit: Has this page been modified, perhaps needing to get written to disk before eviction
  - [Referenced]{.alert} bit: Has the page been referenced; used for eviction
  - [Caching disable]{.alert} bit: Is cache disabled for this page (for direct I/O)
- What doesn't go into one?
  - Backing store addresses: managed by the file system
  - We only want things needed for address translation by hardware

## Paging summary
- Pages allow us to:
  - Load processes at a finer granularity
  - Not experience external fragmentation
  - Swap at page granularity (often termed "paging" rather than "swapping")
- But leads us to have some questions:
  - How do I know which pages are in memory, and which are not?
  - When I need to swap page, which go and which stay?
  - How do I know which pages contain modified data, and I need to write to disk?


## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear?

Comments? Thoughts?
:::

# Page replacement {background-color="#40666e"}

## Questions to consider
:::{.nonincremental}
- What makes a good page replacement strategy, and why?
- How can we approximate optimal page replacement without knowing the future?
- What are the practical implementation challenges for different page replacement algorithms?
:::

## Page replacement policies
- When a running process needs a page, but physical memory is full, we must evict a frame to make room; this is [page replacement]{.alert}
- Which frame do we evict?
- The frame to evict is called the **victim**
  - In some cases, victims may be simply overwritten in memory. When would that make sense?
    - If they are read-only or haven't been modified
  - Or may need to be written to swap (because they have been modified)

## Page replacement
- Keep in mind: Page replacement is a costly operation: at least two I/O operations, so its important to minimize
  - If we're not careful in how we select our victims, we may further delay the system
- Oddly enough, dirty pages may be good victims. Why?
  - Because it has to be written anyways
- Potential downside?
  - It forces the write which may have better optimized with other dirty blocks (potentially disallowing principle of locality benefits)
  - Dirty blocks also are indicative of activity

## Page replacement algorithms: OPT
- Optimal Page Replacement (OPT)
  - We really want to evict the page that *will not be used longest*
  - Each page could be labeled with the number of instructions that will be executed BEFORE page is referenced
  - Of course, this is impossible (akin to SJF as optimal), but useful as a benchmark
  - Example: RAM with 3 slots, following reference pattern:
![](images/pagereplacementpattern.png)
![](images/opt_result.png){.fragment}

## Page replacement algorithms: FIFO
- First-In-First-Out (FIFO)
- Simple: first page in (oldest) will be evicted
- Downsides?
  - Indiscriminate of use: might throw out important pages
- Can lead to [Belady's anomaly]{.alert}: adding more frames can lead to more page faults
  - Why? Because it doesn't consider use at all (the stack property)
  - Try this reference pattern with 3 frames and then 4 frames to see the anomaly:
![](images/beladys.png)

## Page replacement algorithms: LRU
- We can't tell the future, what should we do?
  - Look at the past
- Least Recently Used (LRU)	
  - Each page has some age with respect to use; replace page that hasn't been used in the longest time
  - Like OPT, but only looking backwards
    - Unfortunately, there is a chance that the page you evict is needed immediately

## LRU (cont'd)
- LRU does not exhibit Belady's anomaly
- How can we implement LRU?
  - Counters: each entry has an associated clock
    - No, waste of space, takes time to search
  - Ordered queue	
    - Less space wasted, but a lot more memory accesses (pointers) to keep up-to-date
  - Both are impractical without specialized hardware
  - We can *approximate* LRU with a single bit in the page table entry: **reference bit**	
    - Bit set when page is referenced
    - Bits set to zero initially and on context switch

