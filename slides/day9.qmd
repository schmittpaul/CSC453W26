---
title: CSC 453
subtitle: Winter 2026 Day 9
format: 
  clean-revealjs:
    self-contained: true
    incremental: true
    margin: 0.15 
    # height: 700   # Optional: Normal height (defaults to 700)
    # width: 1000   # Optional: Normal width (defaults to 900)
---

## Admin
:::{.nonincremental}
- Program 3: we need to cover page replacement first (should be Thursday)
  - It's in C or python, your choice
:::

# Handling deadlocks {background-color="#40666e"}

## Questions to consider
:::{.nonincremental}
- How can we detect when a deadlock has occurred?
- What are the trade-offs between different deadlock recovery strategies?
- How should we handle deadlocks that occur through communication rather than resource contention?
:::

## Detecting and recovering from deadlock
- Allow deadlocks to occur, then detect and recover
- In some cases, recovery is pretty simple. If your machine froze once per year (maybe it does?) what would you do?
  - Reboot
- What scenarios do you think complex detection and recovery would make sense?
  - Systems with high availability requirements where manual intervention is costly or impractical
    - telecommunications systems, financial transaction systems, healthcare systems
    - critical infrastructure systems

## Detection
- Use a wait-for graph: a simplified resource-allocation graph that removes resource nodes
  - An edge from process $P_i$ to $P_j$ indicates that $P_i$ is waiting for a resource held by $P_j$
  - A cycle in this graph indicates a deadlock
  - $O(n^2)$ algorithm to search for cycles (beyond the cost of maintaining the graph itself)
  - Only applies to single instance resources

## Detection (cont'd)
![](images/waitforgraph.png)

## Detection (cont'd)
- For multiple instance resources, we can use an algorithm similar to the Banker's algorithm to detect deadlock
- Question: How often should we run the detection algorithm?
  - Too often: wasteful overhead
  - Not often enough: long delays before recovery
  - What if we run it when a process requests a resource?
    - Could allow us to assign blame for deadlock to the requesting process
  - Heuristically (CPU utilization, etc.)
    - Less overhead, but harder to assign blame

## Recovery options
- Process termination
  - Abort all deadlocked processes (brute force)
  - Abort one process at a time until deadlock is resolved
    - How to choose which process to abort?
      - Priority, time running, resources used, etc.
- Resource preemption
  - Temporarily take resources away from some processes and give to others until deadlock is resolved
  - Highly dependent on the resource type
  - Rollback problem - processes may need to be rolled back to a safe state before resources can be reallocated
  - Starvation problem - same victims may be chosen repeatedly

## Communication deadlocks
- Example: A sends a message to B and sleeps until a response; B sleeps until it receives a message; message is lost (packet dropped somewhere)
- This is different than resource deadlock: A does not possess a resource B wants; in the above example, there aren't even any resources
  - yet this still meets our definition of deadlock: a set of processes are waiting for each other in a circular chain
- As such, cannot be prevented using resource-based solution (ordering, preemption, mutual exclusion, etc)
- What should we do?

## Communication deadlocks (cont'd)
- One option: use [timeouts]{.alert} and retries
  - Timers go off after some "expected response" time
    - At what interval? 
    - What do we do when timer goes off? Retransmit? How many times?
  - If there's delay, and not loss, recipient may receive the same message twice
    - What happens in these circumstance?
  - Requires a [protocol]{.alert} for handling (and largely out of the scope of this course)


## Handling deadlocks summary
- When to use which technique?
  - Prevention: when you can afford to limit concurrency and throughput for simplicity
  - Avoidance: when you have good information about process resource needs and can afford the overhead
  - Detection and recovery: when deadlocks are rare and the overhead of prevention/avoidance is unjustified
  - Do nothing: when deadlocks are extremely rare or tolerable and can be handled at the application level
- Depends on **contention**: if resources are highly contended, prevention/avoidance may be necessary to maintain performance

## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear? 

Comments? Thoughts? 
:::

# Memory Fundamentals {background-color="#40666e"}

## Questions to consider
:::{.nonincremental}
- What are the requirements for memory that the OS must accomplish?
- What abstraction should the OS provide to processes?
- What problems does memory virtualization solve?
:::

## Memory overview
- Memory is required by all processes
- What users (developers) want:
  - Infinite space, private, fast, and cheap
- Reality: All memory is limited, only some is fast (and $$$), others are slow (but $)
  - Creates a [memory hierarchy]{.alert}
  - Small, fast, expensive (registers, cache, RAM) â†’ Large, slow, cheap (ssd, disk, tape)
  - In order to support multiprogramming we must share and utilize memory hierarchy intelligently

## Memory hierarchy
![](images/memoryhierarchy.png)

## Memory preliminaries
- Remember: A process's code & data must both be in memory
  - CPU must fetch both as part of its instruction-execution cycle
- What memory can a CPU access directly?
  - Registers, caches, RAM
  - Therefore, any data or instruction must be in these direct access devices in order to operate on them
- In this discussion we're focusing on RAM (caches + registers are hardware problems)
- Memory is an array of words (width of memory), *kind of*

## Memory requirements
- What does the OS need to provide?
- Old days?
  - Just the OS libraries and a single process got the rest of the system memory
- Between multiple processes? Processes and the kernel?
  - [Transparency]{.alert}: we don't want processes to be aware of the virtualization.
    - They need to be presented with a simple address space
  - [Efficiency]{.alert}: memory is expensive and scarce
    - Need to be efficient in both space and time
  - [Protection / isolation]{.alert}: we need to ensure that processes can't access other process's address space (or the kernel's)

## Memory requirements (cont'd)
- From a process point of view how should we address things?
  - Abstraction, we need an **address space**
- We present an address space to processes that is their unique view of the system memory
  - Code, Heap, Stack, etc.
  - Process sees fake (virtual) addresses
  - Q: We know that heap & stack grow towards one another. How does it work when we have multiple threads?

## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear?

Comments? Thoughts?
:::

# Memory Abstraction {background-color="#40666e"}

## Questions to consider
:::{.nonincremental}
- How can the OS provide isolation between processes?
- What are the trade-offs in when we bind virtual addresses to physical addresses?
- How can hardware help the OS manage memory efficiently?
:::

## Base and bounds
- Okay, we're going to lie to processes. How should we achieve this abstraction?
- First approach: base and bounds (or base and limit) registers
  - Base: smallest address; bounds: size of address space
  - This can help with protection / isolation
  - Where should the values be stored?
  - The CPU needs to check for every memory access
    - Stored in the PCB (we just added state that a context switch needs to account for)

## Base and bounds (cont'd)
- What will the OS need to keep track of?
  - When a process is first run
    - Need to find space for it (i.e., must be tracking free space)
  - When a process terminates
    - Return used memory to the free list
    - Clean up any data structures used to manage memory
  - On context switch?
    - CPU only has one set of base and bounds registers
  - When OS wants to relocate a process
    - Set new registers
  - When a process tries to address outside of its bound?
    - exception handlers

## Base and bounds (cont'd)
- Base and bounds is simple, but has some issues:
  - [Internal fragmentation]{.alert}: if a process needs 10KB but we allocate 20KB, we have 10KB of wasted space
  - [External fragmentation]{.alert}: over time, as processes are loaded and unloaded, we can end up with slivers of memory that are not usable for new processes
  - [Relocation is expensive]{.alert}: moving a process in memory requires copying all of its data
- To address these issues, we can use more sophisticated memory management techniques, such as paging and segmentation

## Memory address binding
- When does the OS bind virtual addresses to physical addresses?
  - [Compile time]{.alert}: if we know where the process will be loaded, we can bind addresses at compile time
    - Not flexible
    - if you need to change anything, you need to recompile
  - [Load time]{.alert}: we can bind addresses when the process is loaded into memory
    - More flexible, but still not ideal
    - If you need to move the process, you need to reload it
  - [Runtime]{.alert}: we can bind addresses during execution, which allows for maximum flexibility and is the most common approach
    - Requires some help

## Runtime address binding
- To support runtime address binding, we need to use a [memory management unit (MMU)]{.alert}
  - Hardware component that translates virtual addresses to physical addresses on-the-fly
  - Can also provide protection by checking access rights and ensuring that processes cannot access memory outside of their allocated space
- The OS needs to maintain a data structure (e.g., page table) that the MMU can use for translation
- This allows processes to have a uniform address space (e.g., starting at 0 and going to "max")

## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear?

Comments? Thoughts?
:::

# Memory Optimizations {background-color="#40666e"}

## Questions to consider
:::{.nonincremental}
- How can we reduce memory footprint without limiting functionality?
- What are the benefits and challenges of sharing code across processes?
- What are the trade-offs in swapping processes to disk?
:::

## Dynamic loading
- To further optimize memory usage, we can use dynamic loading
  - Only load parts of a process into memory when they are needed
    - Why would this be required?
      - Otherwise, we'd be restricted to running processes strictly smaller than our physical memory
  - This can be done at the function level (e.g., load a function when it's called) or at the module level (e.g., load a shared library when used)
- Reduces the memory footprint of processes and allows for faster startup times, though it does add complexity
- Any scenarios where this would be particularly beneficial?
  - Good for large, infrequently used routines (e.g., error handling code) or for plugins/extensions that may not be used in every execution

## Dynamic linking
- Instead of statically linking a library at compile time, we can link it at runtime
- Allows multiple processes to share the same library code in memory, reducing overall memory usage (we've already seen this with shared libraries in `pmap` output)
- Allows for libraries to be updated without recompiling processes
- However, it can introduce complexity (e.g., "DLL Hell")
- Does require some support from the OS and the dynamic linker/loader to manage the shared libraries and ensure they are loaded correctly
- What could go wrong here?
  - If a required library is missing or incompatible, the process may fail to start or crash at runtime
  - What if one process updates a shared library while another process is using it? This could lead to unpredictable behavior or crashes

## Swapping
- Swapping allows an entire process to be moved from main memory to a backing store (like disk)
- Q: Why swap?
  - Can run far more processes than RAM can handle
- Q: When do we swap?
  - When we want to run more processes than can fit into physical memory
- Q: When swapping back in, where does the process go?
  - Depends on the memory binding
  - If runtime, then it can go anywhere (maximum flexibility)

## Swapping caveats
- Remember though: disk is very slow, so we must be careful and clever about who, how, and when we swap
  - The amount of time we spend swapping is directly proportional to the amount of memory we want to swap
  - Transfer time dominates (not context switching)
- The state of a process matters
  - E.g. if it's blocked on I/O, we may 1) further delay the I/O by using the disk to swap, 2) a direct I/O request may return to the wrong process
  - Programs are growing larger, so quite expensive to swap
    - e.g. 1GB program ~10 sec per swap to a spinning disk
- In reality: we don't really swap entire processes; too costly and unnecessary
  - We can swap [pages]{.alert}, we'll talk about this later

## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear?

Comments? Thoughts?
:::

# Contiguous allocation {background-color="#40666e"}

## Questions to consider
:::{.nonincremental}
- How do we keep track of free memory and decide where to place processes?
- What problems arise from fragmentation, and how can we address them?
- What are the trade-offs between different allocation strategies?
:::

## Memory allocation strategies
- How do we find free memory for processes?
- How do we keep track of free memory?
- How do we decide which free block to allocate to a process?

## Contiguous allocation
- Memory is allocated in contiguous blocks
- One approach: memory may be partitioned into *fixed-sized partitions*, each containing one process
  - This is very simple, but inflexible
  - A fixed number of partitions means we can only run n processes, and the size of the partitions may not match the needs of the processes
- Another approach: *variable-sized partitions*, where we allocate exactly as much memory as a process needs

## Variable partitioning
- Any obvious problems with variable partitioning?
  - [External fragmentation]{.alert}: over time, as processes are loaded and unloaded, we can end up with small free blocks of memory that are not usable for new processes
- To begin, we allocate memory for processes until none fit into any of the "holes"
- Once there is no hole that is big enough to fit a process, what do we do?
  - Can we rearrange memory to create enough space?
    - [Compaction]{.alert}: move processes around to create enough contiguous space for the new process
    - This is **expensive**

## Allocation strategies
- How do we decide which hole to use for a new process?
- [First fit]{.alert}: allocate the first hole that is big enough
- [Best fit]{.alert}: allocate the smallest hole that is big enough
- [Worst fit]{.alert}: allocate the largest hole
- Downsides to these?
  - First fit: can lead to fragmentation at the beginning of memory
  - Best/worst fit: require full scan
  - One solution: [Next fit]{.alert}: similar to first fit, but continues searching from the last allocated hole

## External fragmentation
- Ideally, we want all memory to be allocated, but
- In reality, we're left with many holes that are not one is big enough to allocate anything into 
  - Although, combined they may be
- All the allocation strategies above can suffer from external fragmentation
- Generally waste 1/3 of the average process size due to external frag using first-fit or best-fit
- One solution: compaction, but this is expensive

## Segmentation
- Previous approaches have been focused on allocating contiguous blocks of memory, but what if we could allow processes to be allocated in non-contiguous segments?
- [Segmentation]{.alert} divides a process's address space into logical segments (e.g., code, data, stack) that can be allocated separately
- Each segment has a base and bounds, and the OS maintains a segment table for each process that maps virtual segment numbers to physical memory locations
- Still suffers from internal fragmentation if segments are not fully utilized
- Can also lead to external fragmentation, but less so than contiguous allocation since segments can be allocated in non-contiguous memory

## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear?

Comments? Thoughts?
:::

