---
title: CSC 453
subtitle: Winter 2026 Day 7
format: 
  clean-revealjs:
    self-contained: true
    incremental: true
    margin: 0.15 
    # height: 700   # Optional: Normal height (defaults to 700)
    # width: 1000   # Optional: Normal width (defaults to 900)
---

## Admin
:::{.nonincremental}
- Break from programming assignments this week
:::


# Thread models {background-color="#40666e"}

## Questions to consider
:::{.nonincremental}
- What is the difference between user threads and kernel threads?
- What are the different kernel thread models and how do they compare?
- How do the different thread models trade off between scheduling control and parallelism?
:::

## First things first
- A kernel thread is the unit of execution that the OS can run on a CPU core
- A process can only make progress if at least one of its kernel threads is scheduled on a core
- CPUs run kernel threads, not processes

## POSIX threads (pthreads)
- 60 calls, but the important ones are: `pthread*`
  - `create`, `exit`, `yield`, `join` (Look familiar?)
- Pthreads were historically (pre 2003) a [user space]{.alert} library: all functionality happens in user space, and the kernel knew nothing about them
  - can run on kernels or CPUs with no thread support
  - orders of magnitude faster than a kernel trap (everything is a local function call)
  - custom schedulers

## Pure user space threads
- Totally unknown to the OS
- User-level threads must "borrow" a kernel thread to run
- What happens when a thread makes an I/O-bound system call?
  - All threads will block, because the process is blocked
  - Sometimes a thread blocking a process is unavoidable: page fault: more later
- Where we typically want threads is where there is a lot of I/O blocking or system calls: the web server example

## Kernel threads
- Thread libraries can also have kernel support (this is the common case today)
- There is no user space runtime environment or thread table: all is managed within the kernel
- Support for multiple cores 
- All thread interfaces are system calls handled by the kernel

## Kernel threads (cont'd)
- Instead of blocking on a system call, the kernel now has the intelligence to schedule another thread
- Creating and destroying threads comes at a higher cost (the kernel trap we were trying to avoid above)
  - What should we do?
    - [Thread pools]{.alert}: a collection of pre-allocated threads, assigned as needed

## Thread design models: many-to-one (M:1)
- Kernel threads must support user level threads. There are multiple ways to pull this off
- [Many-to-one]{.alert}
  - A single kernel level thread supports many user level threads
  - Switching between threads is fast (done in user level)
  - No parallelism (kernel thread is supporting only a single user thread at a time)
  - If any user thread makes an I/O blocking call, all user threads in that process are blocked 
- Where does this make sense?
  - If you need complete control over scheduling
  - If you have no kernel or minimal RTOS kernel

## Thread models: many-to-many (M:N)
- Set of user threads supported by <= number of kernel 
- User thread isn't bound to a particular kernel thread
  - If kernel thread has to block and was supporting 3 user threads, the others can migrate
- Allows parallelism
- Programmer has more control (scheduling in user space)
- Sounds like the best of all worlds, right?
  - Can be hard to implement without decent language support
  - Now you have two schedulers to deal with (user space and kernel)
  - goroutines are a notable example of M:M in reality (though they are achieved through the Go runtime and not the kernel itself)

## Thread models: one-to-one (1:1)

- Number of kernel-level threads is equal to the number of user-level threads (dominant model today)
- Can operate in parallel
- If one blocks, others can continue
- Downsides?
  - Lose the advantage of fast switching between user threads
  - Any new user thread requires kernel thread creation: *expensive*
  - Programmer loses scheduling control: kernel makes decisions
- Very popular model: Linux & Windows
  - Easy to implement
  - Most machines have multiple cores so we can support a lot of kernel threads


## Which models make sense?
- Distributed scientific computing
  - 1:1 - In scientific computing, you want predictable, full-speed access to hardware, not an extra runtime scheduler in the way
- Embedded system
  - M:1 - In embedded systems, simplicity and determinism matter more than scalability. You may only have a single core
- NGINX web servers
  - M:N - NGINX multiplexes connections (user-level work) onto a small number of kernel threads


## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear? 

Comments? Thoughts? 
:::

# Synchronization {background-color="#40666e"}

## Questions to consider
:::{.nonincremental}
- What are the requirements for a critical section synchronization solution?
- Why do naive implementations like a simple boolean lock fail, and what must happen atomically instead?
- What are the practical limitations of Peterson's solution, and why don't they work well on modern architectures?
:::

## Where do we stand?
- Process model allows us an abstraction of a program (and the state to pause it)
- Scheduling allows us concurrency and marshaled access to a scarce resource: CPU
- Mechanisms for process communication and cooperation (shared-memory or message passing)
- Threads allow concurrency within a process and shared memory
  - With shared memory, there is potential for inconsistency

## Problem
- Two processes accessing a reservation system, where the number of open seats is tracked by a counter
  - A: Releasing Seat
  - B: Reserving Seat
- Counter at 10; Process A counter++ = 11; Process B counter-- = 9;
- What could go wrong?
  ```{.c}
  mov 0x8049a1c, %eax
  add $0x1, %eax
  mov %eax, 0x8049a1c

  mov 0x8049a1c, %eax
  sub $0x1, %eax
  mov %eax, 0x8049a1c
  ```

## Race conditions
- If interleaved, whoever goes last wins. Either way, 11 or 9 is an incorrect value. This is a [race condition]{.alert}
- Very common in multi-programming environments: allocating disk blocks or memory pages, writing to a file, network buffers, etc.
- How can we solve race conditions?
  - [Mutual Exclusion]{.alert} (one at a time) on [Critical Sections]{.alert} (the contention data/code)

## Challenges we face
- How large should a critical section be?
- How long can a process be in a critical section
  - What happens if they crash while within a critical section?
- How do we implement? Who enforces entrance into critical sections?

## Beware naïve implementations
- Why not use a simple bool?
  ```{.c}
  bool lock = FALSE
  do {
    while (lock == TRUE);
    lock = TRUE;
    //CRITICAL SECTION
    lock = FALSE;
  }while(TRUE);
  ```

- This implementation requires TWO operations: a read (test) and a write (set) that must be atomic

## Critical section solution requirements
- [Mutual exclusion]{.alert}: only one process can execute at a time
- [Progress]{.alert}: if no process is executing in its critical section and there exist some processes that wish to enter their critical section, then the selection of the process that will enter the critical section next cannot be postponed indefinitely 
  - E.g., A process cannot immediately re-enter the critical section if the other process want to
- [Bounded waiting]{.alert}: In addition to guaranteeing entrance, limits on the amount of time in a critical section must be established
  - No assumptions on the speed or number of CPUs

## Peterson's solution
- Two processes: $P_i$ and $P_j$
- Two shared data items: `int turn; bool flag[2];`
  ```{.c}
  do {
    flag[i] = TRUE;
    turn = j;
    while (flag[j] && turn == j);
    //CRITICAL SECTION
    flag[i] = FALSE;
    //REMAINDER
  }while(TRUE);
  ```
  - turn indicates whose turn it is: `turn == i` → $P_i$'s turn
  - if `flag[i] == true` → $P_i$ is ready for the critical section
- To enter: $P_j$ sets `flag[i] = true` and `turn = j`
  - i.e., $P_i$ demurs to $P_j$; if they are both ready turn will get set to?
    - Whomever runs last sets the turn to the other process 

## Peterson's solution (cont'd)
```{.c}
do {
  flag[i] = TRUE;
  turn = j;
  while (flag[j] && turn == j);
  //CRITICAL SECTION
  flag[i] = FALSE;
  //REMAINDER
}while(TRUE);
```
- Does this meet the three solution requirements?
- Mutual exclusion: under what conditions can a process be in the critical section?
  - $P_i$ only enters if `flag[j]` is false or `turn == i`
  - Conversely, if $P_i$ and $P_j$ are in the CS, then `flag[i] == flag[j]`; this implies both $P_i$ and $P_j$ terminated their whiles at the same time; BUT, in order for this to be true, turn must be both 0 and 1 (Contradiction)

## Peterson's solution (cont'd)
```{.c}
do {
  flag[i] = TRUE;
  turn = j;
  while (flag[j] && turn == j);
  //CRITICAL SECTION
  flag[i] = FALSE;
  //REMAINDER
}while(TRUE);
```
- Progress: Is there a condition where $P_i$ can't enter the CS?
  - There is no condition under which $P_i$ cannot (eventually) enter the CS; either the other process will set `turn == i` or it is already set
- Bounded Waiting: How long will $P_i$ wait in the best/worst case?
  - $P_i$ will enter after *at most* one entry by $P_j$

## Peterson's solution (cont'd)
```{.c}
do {
  flag[i] = TRUE;
  turn = j;
  while (flag[j] && turn == j);
  //CRITICAL SECTION
  flag[i] = FALSE;
  //REMAINDER
}while(TRUE);
```
- Do you see any issues with this solution?
  - Limitation: Only works for two processes
  - Modern architectures may also break this solution
    - Processors and/or compilers can reorder reads and writes that have no dependencies
    - Multithreaded applications could lead to issues

## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear? 

Comments? Thoughts? 
:::

# Primitives {background-color="#40666e"}
## Questions to consider
:::{.nonincremental}
- How do synchronization solutions compare in implementation / function?
- Why can busy-waiting primitives cause problems like priority inversion, and how do blocking primitives help address this?
- When would you choose a counting semaphore versus a binary semaphore or mutex, and what distinguishes them?
:::

## Hardware solutions: atomicity
- One way: use atomic instructions (all-or-none actions)
- Baked into modern hardware
  - `test_and_set()`
  - `compare_and_swap()`
  - These both lock the memory bus (lighter weight than the olden days when we could alternatively disable interrupts)
    - Disabling interrupts only really works on a single CPU, modern architectures make it untenable
- Easy enough for simple operations
- Not realistic for complex operations
  - Ex. "atomic update of a B-tree": do we really want to bake such an instruction into the hardware? No

## Hardware solutions: atomicity (cont'd)
```{.c}
bool test_and_set(bool *lock){
  bool ret = *lock;
  *lock = TRUE;
  return ret;
}

lock = FALSE;
do{
  while(test_and_set(&lock));
  // CRITICAL SECTION
  lock = FALSE;
}while(true)
```
- Does this `test_and_set()` solution meet the requirements?
- Mutual exclusion?
  - Yes
- Bounded wait/Progress?
  - No: you could end up with a situation where an unlucky process is endlessly waiting based on the scheduler's decisions

## Mutex locks
- Previous solutions are complicated and generally not used by application programmers (kernel hackers only)
- Any solution to the critical section problem requires a lock
  - Peterson's solution is an example (in software)
  - `test_and_set()` / `compare_and_swap()` are hardware implementations
- Simplest commonly used solution is [mutex]{.alert} lock
  - Boolean variable indicating if lock is available or not

## Mutex locks (cont'd)
- Protect a critical section  by 
  - First `acquire()` a lock 
  - Then `release()` the lock
- What's going on under the hood? 
  ```{.c}
  void lock(lock_t *lock) {
    while (test_and_set(&lock->flag, 1) == 1)
    ; // spin-wait (do nothing)
  }
  ```
  - `test_and_set()` or `compare_and_swap()`

## Mutex locks (cont'd)
- Calls to `acquire()` and `release()` must be [atomic]{.alert}
- But this solution requires **busy waiting**
  - This type of mutex is therefore often called a [spinlock]{.alert}
    - NOT ALWAYS TERRIBLE: spinlocks are preferable in some cases. When do you think?
      - When the lock is likely to be held for <= the time for two context switches
- There are also mutex implementations (e.g., futex) that block to avoid busy-waiting, and some that do both
- **Note**: mutexes are owned by the thread that locks them. *Only the thread that acquired the mutex can release it*

## Busy waiting consequence
In 1997, NASA's Mars Pathfinder started having mysterious resets on Mars. From Earth, engineers saw the system repeatedly rebooting for no obvious reason. Not ideal when your computer is 250 million km away.

The cause turned out to be [priority inversion]{.alert} in the operating system.

- What was happening:
  - A low-priority task was holding a shared resource (a mutex).
	- A high-priority task needed that resource and blocked, waiting.
	- Meanwhile, a medium-priority task—which didn't need the resource at all—kept preempting the low-priority task.
	- Result: the low-priority task never got CPU time to release the resource, so the high-priority task was effectively starved.

- The system interpreted this long delay as a fault and triggered a watchdog reset. Over and over.

## Priority inversion
- Any ideas on how to fix?
  - [Priority inheritance]{.alert}: low priority processes are temporarily given priorities that match those waiting on them

## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear? 

Comments? Thoughts? 
:::

## Semaphores
:::: {.columns}
::: {.column width="68%" .nonincremental}
- Synchronization tool that provides more sophisticated ways (than mutex locks)  for processes to synchronize their activities
- Semaphore `S`: integer variable
- Can only be accessed via two atomic operations
  - `wait()` and `signal()`
    - Originally called `P()` and `V()` (because, Dutch - another Dijkstra invention)
- Do not have ownership, any thread can signal (increment) or wait (decrement)
:::

::: {.column width="2%"}
:::

::: {.column width="30%"}
```{.c}
wait(S) { 
    while (S <= 0)
       ; // busy
    S--;
}

signal(S) { 
    S++;
}
```
:::
::::

## Semaphores (cont'd)
:::: {.columns}
::: {.column width="68%" .nonincremental}
- [Binary semaphore]{.alert}: integer value can range only between 0 and 1
  - Functionally the same as a mutex (often when people call something a mutex it's actually a binary semaphore)


:::

::: {.column width="2%"}
:::

::: {.column width="30%"}
```{.c}
do{
  wait(mutex);
  //CS
  signal(mutex);
}while(TRUE);
```
:::
::::
- [Counting semaphore]{.alert}: integer value can range over an unrestricted domain
  - When would this make sense?
    - Protect a finite set of resources rather than a single one (e.g., you have a pool of db connections that can be shared by processes)
    - Initialize `S` to the number of resources
    - Decrement `S` each time someone grabs a resource, increment when they release

## Semaphores (cont'd)
:::: {.columns}
::: {.column width="68%" .nonincremental}
- Can also be used to force synchronization
- Consider $P_1$  and $P_2$ that with two statements $S_1$ and $S_2$ and the requirement that $S_1$ to happen before $S_2$

:::

::: {.column width="2%"}
:::

::: {.column width="30%"}
```{.c}
P1:
   S1;
   signal(sync);
P2:
   wait(sync);
   S2;
```
:::
::::

## Blocking semaphores
:::: {.columns}
::: {.column width="58%" .nonincremental}
- We add `sleep()` and `wakeup()` to the underlying implementation
- Call to wait, and semaphore not available, process is blocked with `sleep()` (wait state) and placed on a waiting queue
- Blocked processes are notified of an available semaphore by the `wakeup()` operation
  - goes from waiting to ready
  - Able to achieve bounded wait and progress with FIFO queue

:::

::: {.column width="2%"}
:::

::: {.column width="40%"}
```{.c}
wait(semaphore *S) { 
   S->value--; 
   if (S->value < 0) {      
      add process to S->list; 
      sleep(); 
   } 
}

signal(semaphore *S) { 
   S->value++; 
   if (S->value <= 0) {      
      remove P from S->list; 
      wakeup(P); 
   } 
} 

```
:::
::::

## Mutexes vs. semaphores
- Mutexes are generally lighter weight / faster
- Semaphores can support multiple instances
- Semaphores don't have the ownership limitations 

## Choose your primitive
- For each, choose between: atomic instructions; futexes; spin lock mutexes; semaphores
- Scenario 1: You need to protect a critical section where only one thread should execute at a time, and the critical section is expected to be held for a relatively long duration.
  - Futex
- Scenario 2: You have a pool of database connections, and you need to limit the number of threads that can access these connections simultaneously.
  - Counting semaphore

## Choose your primitive (cont'd)
- For each, choose between: atomic instructions; futexes; spin lock mutexes; semaphores
- Scenario 3: You need to increment a shared counter frequently, and the operation is very quick.
  - Atomic instruction
- Scenario 4: You need to protect a critical section where only one thread should execute at a time, but the critical section is expected to be held for a very short duration.
  - Spin lock mutex

## Beware: mistakes are easy to make
-  Incorrect use of semaphore operations
    - wrong order: `signal(mutex)` ...  `wait(mutex)`
    - repeating: `wait(mutex)`  ...  `wait(mutex)`
    - Omitting of `wait(mutex)` and/or `signal(mutex)`
- Be careful

## Monitors
- Subtle mistakes with semaphores can lead to deadlocks, leading to [monitors]{.alert}
- Can think of monitors as a library with an API (abstract data type)
  - Processes share the library, but not internal data (directly)
  - This requires language specific understanding of a monitor (C doesn't have them, natively)
  - More relevant in languages like Java and other high-level languages that provide built-in monitor support

## Monitors (cont'd)
:::: {.columns}
::: {.column width="58%" .nonincremental}
- Main idea: only one processes can be active within a monitor at any instant
  - Up to the compilers to ensure mutual exclusion on monitor procedures
    - It can use other sync primitives to achieve this: e.g. a semaphore or mutex
  - We're offloading synchronization correctness to the compiler, (hopefully) lessening the chance for error by the user
- Monitor variables are private
:::

::: {.column width="2%"}
:::

::: {.column width="40%"}
![](images/monitor.png)
:::
::::

## Condition variables
- Monitors often have condition variables
- Condition variables have `wait()` and `signal()` operations
  - `x.wait()`:  a process that invokes the operation is suspended until `x.signal()` 
  - `x.signal()`: resumes one of processes (if any) that invoked `x.wait()`
    - If no `x.wait()` on the variable, then it has no effect on the variable
- Big benefit of condition variables: `x.broadcast()`

## Mutexes, semaphores, Monitors, Condition variables
- Mutexes and Semaphores are good for:
  - Simple mutual exclusion
  - Simple counting resources
- Monitors are good for:
  - Complex synchronization between many threads
  - Thread-safe operations (only allow one thread into the monitor at a time)
- Condition variables are good for:
  - Producer-consumer problems (consumer threads waiting on some condition, signaled when ready)
  - We want to make one or more threads sleep until a resource is ready


## {background-color="#6E404F"}
::: {.r-fit-text}
What isn't clear? 

Comments? Thoughts? 
:::
